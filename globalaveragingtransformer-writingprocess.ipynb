{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch.nn as nn\nimport torch\nfrom torch import optim\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-21T03:47:22.305272Z","iopub.execute_input":"2023-10-21T03:47:22.306205Z","iopub.status.idle":"2023-10-21T03:47:22.316477Z","shell.execute_reply.started":"2023-10-21T03:47:22.306167Z","shell.execute_reply":"2023-10-21T03:47:22.315418Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ***Competition Overview:***\n\n**Dataset:** The competition dataset comprises approximately 5000 logs of user inputs. These logs are generated while users are composing essays.\n**Task:** The goal of this competition is to predict the score that an essay received based on the log of user inputs. The scores are on a scale of 0 to 6, indicating the quality or effectiveness of the essay.\n**File and Field Information:**\nThe competition provides a CSV file called train_logs.csv, which contains the following fields:\n\n* id: This is a unique identifier for each essay.\n* event_id: An index indicating the order of events in the log, ordered chronologically.\n* down_time: The time when the down event (e.g., keypress or mouse click) occurred, measured in milliseconds.\n* up_time: The time when the up event (e.g., key release or mouse release) occurred, measured in milliseconds.\n* action_time: The duration of the event, which is the difference between down_time and up_time.\n* activity: The category of activity that the event belongs to. It can have values like \"Nonproduction,\" \"Input,\" \"Remove/Cut,\" \"Paste,\" \"Replace,\" or \"Move From [x1, y1] To [x2, y2]\".\n* down_event: The name of the event when the key or mouse is pressed.\n* up_event: The name of the event when the key or mouse is released.\n* text_change: The text that changed as a result of the event (if any). This field represents the alteration made to the essay text.\n* cursor_position: The character index of the text cursor after the event.\n* word_count: The word count of the essay after the event.\n\n**Objective:**\nParticipants in this competition are tasked with using the provided log data to build a predictive model. This model should take the log events as input and predict the essay's score on the 0 to 6 scale.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\ndf_score = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:22.318179Z","iopub.execute_input":"2023-10-21T03:47:22.318483Z","iopub.status.idle":"2023-10-21T03:47:25.594224Z","shell.execute_reply.started":"2023-10-21T03:47:22.318458Z","shell.execute_reply":"2023-10-21T03:47:25.590888Z"},"trusted":true},"execution_count":33,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_score \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."],"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error"}]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.594909Z","iopub.status.idle":"2023-10-21T03:47:25.595282Z","shell.execute_reply.started":"2023-10-21T03:47:25.595100Z","shell.execute_reply":"2023-10-21T03:47:25.595117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Activity \n* Nonproduction - The event does not alter the text in any way\n* Input - The event adds text to the essay\n* Remove/Cut - The event removes text from the essay\n* Paste - The event changes the text through a paste input\n* Replace - The event replaces a section of text with another string\n* Move From [x1, y1] To [x2, y2] - The event moves a section of text spanning character index x1, y1 to a new location x2, y2","metadata":{}},{"cell_type":"code","source":"def move_from(x):\n    if 'Move From' in x:\n        return 'Move From'\n    else:\n        return x ","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.596392Z","iopub.status.idle":"2023-10-21T03:47:25.596836Z","shell.execute_reply.started":"2023-10-21T03:47:25.596629Z","shell.execute_reply":"2023-10-21T03:47:25.596652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.activity  = df['activity'].apply(lambda x : move_from(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.598628Z","iopub.status.idle":"2023-10-21T03:47:25.598937Z","shell.execute_reply.started":"2023-10-21T03:47:25.598784Z","shell.execute_reply":"2023-10-21T03:47:25.598799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.activity  = test['activity'].apply(lambda x : move_from(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.600162Z","iopub.status.idle":"2023-10-21T03:47:25.600482Z","shell.execute_reply.started":"2023-10-21T03:47:25.600329Z","shell.execute_reply":"2023-10-21T03:47:25.600344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = 100\n# Count the number of event_ids for each essay_id\ncounts = df.groupby('id').size().sort_values()\nprint(counts)\n# Split the dataframe based on the condition\ngroups = []\ncurrent_group = []\ncurrent_min = counts.iloc[0]\ncurrent_max = current_min\n\nfor essay, count in counts.items():\n    if (current_max - current_min) <= seq_len:\n        current_group.append(essay)\n        current_max = max(current_max, count)\n    else:\n        groups.append(current_group)\n        current_group = [essay]\n        current_min = count\n        current_max = count\n\nif current_group:\n    groups.append(current_group)\n\n# Split the dataframe into multiple dataframes based on the groups\ndfs = [df[df['id'].isin(group)] for group in groups]\n\nprint(f\"num buckets: {len(dfs)}\")\n# Print the resulting dataframes\nfor group_df in dfs:\n    print(group_df)\n    print(\"------\")","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.601458Z","iopub.status.idle":"2023-10-21T03:47:25.601758Z","shell.execute_reply.started":"2023-10-21T03:47:25.601608Z","shell.execute_reply":"2023-10-21T03:47:25.601622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Essay Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\n\ndef pad_collate_fn(batch):\n    # Extract sequences and targets from the batch\n    sequences, targets = zip(*batch)\n    \n    # Determine the maximum sequence length in this batch\n    max_length = max([seq.size(0) for seq in sequences])\n    \n    # Initialize list to store padded sequences\n    padded_sequences = []\n    \n    for seq in sequences:\n        # Calculate the padding length for this sequence\n        padding_length = max_length - seq.size(0)\n        \n        # Create a zero-filled tensor for padding\n        padding_tensor = torch.zeros((padding_length, 4))\n        \n        # Append the padding to the original sequence\n        padded_seq = torch.cat([seq, padding_tensor], dim=0)\n        \n        # Add the padded sequence to the list\n        padded_sequences.append(padded_seq)\n    \n    # Convert lists of tensors into tensors\n    return torch.stack(padded_sequences), torch.tensor(targets)\n\nclass EssayDataset(Dataset):\n    def __init__(self, train_df, score_df):\n        self.train_df = train_df\n        self.score_df = score_df\n        self.unique_ids = train_df['id'].unique()\n        self.columns_to_return = ['action_time', 'activity', 'down_event', 'word_count']\n        self.down_event_to_index = {event: idx for idx, event in enumerate(unique_down_events)}\n        self.unidentified_de_index = self.down_event_to_index.get('Unidentified', len(unique_down_events))\n        self.activity_to_index =  {act: idx for idx, act in enumerate(unique_activities)}\n        self.unidentified_a_index = self.activity_to_index.get('Unidentified', len(unique_activities))\n        self.score_to_index =  {score: idx for idx, score in enumerate(unique_scores)}\n        \n\n\n    def __len__(self):\n        return len(self.unique_ids)\n\n    def __getitem__(self, idx):\n        essay_id = self.unique_ids[idx]\n        essay_data = self.train_df[self.train_df['id'] == essay_id]\n        essay_data = essay_data.sort_values(by='event_id')[self.columns_to_return]\n        \n        # Normalize and replace categories with indices\n        essay_data['action_time'] = (essay_data['action_time'] - essay_data['action_time'].mean()) / essay_data['action_time'].std()\n        essay_data['word_count'] = (essay_data['word_count'] - essay_data['word_count'].mean()) / essay_data['word_count'].std()\n        essay_data['down_event'] = essay_data['down_event'].map(self.down_event_to_index).fillna(self.unidentified_de_index).astype(int)\n        essay_data['activity'] = essay_data['activity'].map(self.activity_to_index).fillna(self.unidentified_a_index).astype(int)\n        \n        essay_score = self.score_to_index[self.score_df[self.score_df['id'] == essay_id]['score'].values[0]]\n        essay_tensor = torch.Tensor(essay_data.values)\n        return essay_tensor, essay_score","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:55:58.459209Z","iopub.execute_input":"2023-10-21T04:55:58.459581Z","iopub.status.idle":"2023-10-21T04:55:58.474791Z","shell.execute_reply.started":"2023-10-21T04:55:58.459552Z","shell.execute_reply":"2023-10-21T04:55:58.473822Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"\n\nunique_down_events = list(df['down_event'].unique())\nunique_activities = list(df['activity'].unique())\nunique_scores = list(df_score['score'].unique())\nprint(unique_scores)\nunique_activities.append('Unidentified')\nbatch_size = 64\ntrain_loaders = []\nval_loaders = []\nfor bucket_df in dfs:\n    dataset = EssayDataset(bucket_df, df_score)\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn)\n    train_loaders.append(train_loader)\n    val_loaders.append(val_loader)\n\nprint(len(train_loaders))\nprint(len(val_loaders))","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:55:58.978512Z","iopub.execute_input":"2023-10-21T04:55:58.978845Z","iopub.status.idle":"2023-10-21T04:56:00.808017Z","shell.execute_reply.started":"2023-10-21T04:55:58.978820Z","shell.execute_reply":"2023-10-21T04:56:00.807056Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"[3.5, 6.0, 2.0, 4.0, 4.5, 2.5, 5.0, 3.0, 1.5, 5.5, 1.0, 0.5]\n73\n73\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Input Embeddings\n","metadata":{}},{"cell_type":"code","source":"class InputEssayEmbeddings(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n\n        # Embedding for the categorical columns\n        self.embed_activity = nn.Embedding(len(unique_activities), (n_embed-2)//2)\n        self.embed_down_event = nn.Embedding(len(unique_down_events), (n_embed-2)//2)\n        \n        # activities: 6\n        # down_event: 131\n\n    def forward(self, input_tensor):\n        # Slice columns from the input tensor\n        action_time_col = input_tensor[:, :, 0:1]  # [B, num_events, 1]\n        activity_col = input_tensor[:, :, 1].long()  # [B, num_events]\n        down_event_col = input_tensor[:, :, 2].long()    # [B, num_events]\n        word_count_col = input_tensor[:, :, 3:4]  # [B, num_events, 1]\n\n        # Pass through respective embedding layers\n        embed_down_event = self.embed_down_event(down_event_col)  # [B, num_events, embed_dim_down_event]\n        embed_activity = self.embed_activity(activity_col)        # [B, num_events, embed_dim_activity]\n        \n        # Concatenate them with the rest of the tensor\n        out = torch.cat([action_time_col, embed_activity, embed_down_event, word_count_col], dim=2)\n        return out # [B, num_events, n_embed]\n    \nclass PositionalEncoding(nn.Module):\n    def __init__(self, n_embed, seq_len):\n        super().__init__()\n        # Dict size\n        self.emb = nn.Embedding(seq_len, n_embed)\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.device = torch.device(device)\n\n    def forward(self, x, batched=False):\n        \"\"\"\n        :param x: If using batching, should be [batch size, seq len, embedding dim]. Otherwise, [seq len, embedding dim]\n        :return: a tensor of the same size with positional embeddings added in\n        \"\"\"\n        # Second-to-last dimension will always be sequence length\n        input_size = x.shape[-2]\n        indices_to_embed = torch.tensor(np.asarray(range(0, input_size))).type(torch.LongTensor).to(self.device)\n        if batched:\n            # Use unsqueeze to form a [1, seq len, embedding dim] tensor -- broadcasting will ensure that this\n            # gets added correctly across the batch\n            emb_unsq = self.emb(indices_to_embed).unsqueeze(0)\n            return x + emb_unsq\n        else:\n            return x + self.emb(indices_to_embed)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:56:02.430379Z","iopub.execute_input":"2023-10-21T04:56:02.430747Z","iopub.status.idle":"2023-10-21T04:56:02.442955Z","shell.execute_reply.started":"2023-10-21T04:56:02.430719Z","shell.execute_reply":"2023-10-21T04:56:02.441988Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# Define Architecture Sub Components","metadata":{}},{"cell_type":"code","source":"class FeedFoward(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, 4 * n_embed),\n            nn.ReLU(),\n            nn.Linear(4 * n_embed, n_embed),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n    \nclass Head(nn.Module):\n    def __init__(self, seq_length, n_embed, num_heads, n_internal):\n        super().__init__()\n        self.K = nn.Linear(n_embed, n_internal)\n        self.Q = nn.Linear(n_embed, n_internal)\n        self.V = nn.Linear(n_embed, n_internal)\n        self.w0 = nn.Linear(n_internal, n_embed // num_heads)\n        \n\n    def forward(self, input_vecs):\n        keys = self.K(input_vecs) # B, L, d_internal\n        d_k = keys.shape[-1]\n        queries = self.Q(input_vecs) # B, L, d_internal\n        value = self.V(input_vecs) # B, L, d_internal\n        \n        weights = torch.matmul(queries, keys.transpose(-2, -1)) * d_k**-0.5# L, L\n        attention = torch.softmax(weights, dim=-1)\n\n        logit = torch.matmul(attention , value) # B, L, d_internal\n        logit = self.w0(logit)\n        return logit\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, seq_length, n_embed, num_heads, n_internal):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(seq_length, n_embed, num_heads, n_internal) for _ in range(num_heads)])\n        \n        \n    def forward(self, input_vecs):\n        cls_tokens = []\n        for head in self.heads:\n            head_out = head(input_vecs)  \n            cls_tokens.append(head_out[:, 0])\n        cls_tokens_cat = torch.cat(cls_tokens, dim=-1)\n        #print(cls_tokens[0].shape)\n        return cls_tokens_cat.unsqueeze(1) # B, n_embed\n\nclass MHAConvolution(nn.Module):\n    def __init__(self, seq_length, n_embed, num_heads, n_internal, stride=1):\n        super().__init__()\n        self.stride = stride\n        self.window_size = seq_length\n        self.n_heads = num_heads\n        self.cls_token = nn.Parameter(torch.randn(1, 1, n_embed)) \n        self.pos_embedding = PositionalEncoding(n_embed, seq_length)\n        self.multi_head_attention = MultiHeadAttention( seq_length+1, n_embed, num_heads, n_internal)\n        self.ffwd = FeedFoward(n_embed)\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n\n    def forward(self, input_vecs, batched=False): # B, long_seq_len, n_embed\n        \n        outputs = []\n        #print(input_vecs.shape)\n        for i in range(0, input_vecs.size(1) - self.window_size + 1, self.stride): # TODO: think about ways to downsample other than stride\n            # prepend the cls token to the input\n            local_window = input_vecs[:, i:i+self.window_size, :]  # [B, seq_length, embed_size]\n            B, L, _ = local_window.size()\n            cls_tokens_repeated = self.cls_token.repeat(B, 1, 1) # B, 1, n_embed\n            local_window = self.pos_embedding(local_window, batched=batched)\n            local_window_cls = torch.cat([cls_tokens_repeated, local_window], dim=1)\n            attention_out = self.multi_head_attention(self.ln1(local_window_cls)) # B, 1, n_embed, a single cls vector cated from all heads\n            attention_out += cls_tokens_repeated # residual for the cls_tokens\n            out = attention_out + self.ffwd(self.ln2(attention_out))\n            out = out.view(B, -1) # B, n_heads*n_embed\n            outputs.append(out) \n        final = torch.stack(outputs, dim=1)\n        #print(final.shape)\n        return final # B, (long_seq_len-seq_len)/stride, n_embed\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:56:04.368383Z","iopub.execute_input":"2023-10-21T04:56:04.369246Z","iopub.status.idle":"2023-10-21T04:56:04.389584Z","shell.execute_reply.started":"2023-10-21T04:56:04.369215Z","shell.execute_reply":"2023-10-21T04:56:04.388534Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"# Define the Model","metadata":{}},{"cell_type":"code","source":"class GlobalAveragingTransformer(nn.Module):\n    def __init__(self, seq_len, n_embed, n_internal, num_heads, n_scores):\n        super().__init__()\n        self.seq_len = seq_len\n        self.mha_conv = MHAConvolution(seq_len, n_embed, num_heads, n_internal, stride=seq_len//2)\n        self.classifier = nn.Linear(n_embed, n_scores) # consider adding intermediat ffw layers\n        self.embedding = InputEssayEmbeddings(n_embed)\n        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n        \n    def forward(self, input_tensor, batched=False):\n        x = self.embedding(input_tensor)\n        x = self.mha_conv(x) # B, down_sampled_len, n_embed\n        x_permuted = x.permute(0, 2, 1) # B, n_embed, down_sampled_len\n        x = self.adaptive_pool(x_permuted).squeeze(-1) # B, n_embed\n        #print(x.shape)\n        x = self.classifier(x) # B, n_scores\n        if batched:\n            return x\n        else:\n            return x.squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:56:05.338750Z","iopub.execute_input":"2023-10-21T04:56:05.339104Z","iopub.status.idle":"2023-10-21T04:56:05.347218Z","shell.execute_reply.started":"2023-10-21T04:56:05.339076Z","shell.execute_reply":"2023-10-21T04:56:05.346179Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"model = GlobalAveragingTransformer(100, 10, 5, 2, len(unique_scores))\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device)\n\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:56:05.994112Z","iopub.execute_input":"2023-10-21T04:56:05.994476Z","iopub.status.idle":"2023-10-21T04:56:06.006171Z","shell.execute_reply.started":"2023-10-21T04:56:05.994450Z","shell.execute_reply":"2023-10-21T04:56:06.005179Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"GlobalAveragingTransformer(\n  (mha_conv): MHAConvolution(\n    (pos_embedding): PositionalEncoding(\n      (emb): Embedding(100, 10)\n    )\n    (multi_head_attention): MultiHeadAttention(\n      (heads): ModuleList(\n        (0-1): 2 x Head(\n          (K): Linear(in_features=10, out_features=5, bias=True)\n          (Q): Linear(in_features=10, out_features=5, bias=True)\n          (V): Linear(in_features=10, out_features=5, bias=True)\n          (w0): Linear(in_features=5, out_features=5, bias=True)\n        )\n      )\n    )\n    (ffwd): FeedFoward(\n      (net): Sequential(\n        (0): Linear(in_features=10, out_features=40, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=40, out_features=10, bias=True)\n      )\n    )\n    (ln1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n    (ln2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=10, out_features=12, bias=True)\n  (embedding): InputEssayEmbeddings(\n    (embed_activity): Embedding(7, 4)\n    (embed_down_event): Embedding(131, 4)\n  )\n  (adaptive_pool): AdaptiveAvgPool1d(output_size=1)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Run Training Loop","metadata":{}},{"cell_type":"code","source":"max_epochs = 40\nscheduler_enabled = False\n\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)\ncriterion = torch.nn.CrossEntropyLoss().to(device)\nmodel.train()\ndataset_size = len(train_loader)\nprint(\"Starting training loop\")\nprint(len(train_loaders))\nnum_buckets = len(train_loaders)\nfor epoch in range(max_epochs):\n    print(f\"Training epoch {str(epoch)}\")\n    num_correct = 0\n    num_samples = 0\n    total_loss = 0.0\n    i = 0\n    for bucket_train_loader in train_loaders:\n        #print(f\"training bucket {i}...\")\n        train_loader = bucket_train_loader\n        \n        for batch_x, batch_y in train_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n            optimizer.zero_grad()\n            logit = model.forward(batch_x, batched=True)\n            #print(logit.shape)\n            #print(batch_y.shape)\n            loss = criterion(logit, batch_y)\n            total_loss += loss\n            #train_logger.add_scalar('loss', loss, epoch * dataset_size + i)\n            _, pred_labels = torch.max(logit, dim=1)\n            num_correct += (pred_labels == batch_y).sum().item()\n            num_samples += batch_y.size(0)\n\n            loss.backward()\n            optimizer.step()\n        #print(f\"finished bucket {i} out of {num_buckets} with loss {total_loss/num_buckets}\")\n        i+=1\n        \n\n    acc = 100 * num_correct / num_samples\n    print(f\"training epoch {epoch} finished with acc {acc} and total loss {total_loss}\")\n    #train_logger.add_scalar('accuracy', acc, epoch * dataset_size + i - 1)\n    \n        \n    model.eval()\n    num_correct = 0\n    num_samples = 0\n    with torch.no_grad():\n        i = 0\n        for bucket_val_loader in val_loaders:\n            #print(f\"validating bucket {i}...\")\n            validation_data_loader = bucket_val_loader\n            for batch_x, batch_y in validation_data_loader:\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n                logits = model(batch_x, batched=True)\n                #print(logits.shape)\n                _, predicted = torch.max(logits, dim=1)\n                num_samples += batch_y.size(0)\n                num_correct += (predicted == batch_y).sum().item()\n            i += 1\n            if i > 10:\n                break\n    acc = 100 * num_correct / num_samples\n    print(f\"validation epoch {epoch} finished with acc {acc}\")\n    if scheduler_enabled:\n        scheduler.step(acc)\n    #valid_logger.add_scalar('accuracy', acc, epoch * dataset_size + i - 1)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-21T04:56:07.464335Z","iopub.execute_input":"2023-10-21T04:56:07.464723Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Starting training loop\n73\nTraining epoch 0\ntraining epoch 0 finished with acc 5.295629820051414 and total loss 194.32501220703125\nvalidation epoch 0 finished with acc 0.0\nTraining epoch 1\ntraining epoch 1 finished with acc 14.55012853470437 and total loss 181.71363830566406\nvalidation epoch 1 finished with acc 0.0\nTraining epoch 2\ntraining epoch 2 finished with acc 15.886889460154242 and total loss 177.4959259033203\nvalidation epoch 2 finished with acc 0.0\nTraining epoch 3\n","output_type":"stream"}]},{"cell_type":"code","source":"df_agg = df.groupby(['id','activity']).agg({\n    'event_id': np.max,\n    'action_time' : [np.mean, np.sum, np.min, np.max],\n    'word_count' : np.max,\n    'cursor_position' : np.max\n}).reset_index()\ndf_agg.head(10)\n#df_agg.colum1ns = [i+\"_\"+j for i,j in df_agg.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.614208Z","iopub.status.idle":"2023-10-21T03:47:25.614502Z","shell.execute_reply.started":"2023-10-21T03:47:25.614353Z","shell.execute_reply":"2023-10-21T03:47:25.614367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.groupby(['id','activity']).agg({\n    'event_id': np.max,\n    'action_time' : [np.mean, np.sum, np.min, np.max],\n    'word_count' : np.max,\n    'cursor_position' : np.max\n}).reset_index()\n\ntest.columns = [i+\"_\"+j for i,j in test.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.618084Z","iopub.status.idle":"2023-10-21T03:47:25.618449Z","shell.execute_reply.started":"2023-10-21T03:47:25.618288Z","shell.execute_reply":"2023-10-21T03:47:25.618304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat = ['event_id_amax', 'action_time_mean',\n       'action_time_sum', 'action_time_amin', 'action_time_amax',\n       'word_count_amax', 'cursor_position_amax']\ndf_pvt = pd.pivot_table(df_agg, values =feat, index =['id_'],\n                         columns =['activity_'], aggfunc = np.max).reset_index()\n\ndf_pvt.columns = [i+\"_\"+j for i,j in df_pvt.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.620029Z","iopub.status.idle":"2023-10-21T03:47:25.620374Z","shell.execute_reply.started":"2023-10-21T03:47:25.620212Z","shell.execute_reply":"2023-10-21T03:47:25.620228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remaining_cols = list(set(feat) - set(test.columns))\nif len(remaining_cols) != 0:\n    for i in remaining_cols:\n        test[i] = 0","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.621757Z","iopub.status.idle":"2023-10-21T03:47:25.622064Z","shell.execute_reply.started":"2023-10-21T03:47:25.621907Z","shell.execute_reply":"2023-10-21T03:47:25.621921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.pivot_table(test, values = feat, index =['id_'],\n                         columns =['activity_'], aggfunc = np.max).reset_index()\n\ntest.columns = [i+\"_\"+j for i,j in test.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.623311Z","iopub.status.idle":"2023-10-21T03:47:25.623636Z","shell.execute_reply.started":"2023-10-21T03:47:25.623477Z","shell.execute_reply":"2023-10-21T03:47:25.623492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remaining_cols = list(set(df_pvt.columns) - set(test.columns))\nif len(remaining_cols) != 0:\n    for i in remaining_cols:\n        test[i] = 0","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.624887Z","iopub.status.idle":"2023-10-21T03:47:25.625339Z","shell.execute_reply.started":"2023-10-21T03:47:25.625095Z","shell.execute_reply":"2023-10-21T03:47:25.625116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[df_pvt.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.627265Z","iopub.status.idle":"2023-10-21T03:47:25.627594Z","shell.execute_reply.started":"2023-10-21T03:47:25.627433Z","shell.execute_reply":"2023-10-21T03:47:25.627448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[df_pvt.columns]\ndf_pvt = df_pvt.rename(columns={'id__':'id'})\ntest = test.rename(columns={'id__':'id'})\ndf_pvt = df_pvt.merge(df_score, on = 'id', how = 'left')","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.628960Z","iopub.status.idle":"2023-10-21T03:47:25.629304Z","shell.execute_reply.started":"2023-10-21T03:47:25.629120Z","shell.execute_reply":"2023-10-21T03:47:25.629135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(df_pvt['score'])\nplt.figure(figsize=(15, 7))\nsns.heatmap(df_pvt.drop(['id'],axis= 1).corr(), annot=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.630274Z","iopub.status.idle":"2023-10-21T03:47:25.630683Z","shell.execute_reply.started":"2023-10-21T03:47:25.630483Z","shell.execute_reply":"2023-10-21T03:47:25.630503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['action_time_amax_Input', 'action_time_amax_Move From',\n       'action_time_amax_Nonproduction', 'action_time_amax_Paste',\n       'action_time_amax_Remove/Cut', 'action_time_amax_Replace',\n       'action_time_amin_Input', 'action_time_amin_Move From',\n       'action_time_amin_Nonproduction', 'action_time_amin_Paste',\n       'action_time_amin_Remove/Cut', 'action_time_amin_Replace',\n       'action_time_mean_Input', 'action_time_mean_Move From',\n       'action_time_mean_Nonproduction', 'action_time_mean_Paste',\n       'action_time_mean_Remove/Cut', 'action_time_mean_Replace',\n       'action_time_sum_Input', 'action_time_sum_Move From',\n       'action_time_sum_Nonproduction', 'action_time_sum_Paste',\n       'action_time_sum_Remove/Cut', 'action_time_sum_Replace',\n       'cursor_position_amax_Input', 'cursor_position_amax_Move From',\n       'cursor_position_amax_Nonproduction', 'cursor_position_amax_Paste',\n       'cursor_position_amax_Remove/Cut', 'cursor_position_amax_Replace',\n       'event_id_amax_Input', 'event_id_amax_Move From',\n       'event_id_amax_Nonproduction', 'event_id_amax_Paste',\n       'event_id_amax_Remove/Cut', 'event_id_amax_Replace',\n       'word_count_amax_Input', 'word_count_amax_Move From',\n       'word_count_amax_Nonproduction', 'word_count_amax_Paste',\n       'word_count_amax_Remove/Cut', 'word_count_amax_Replace',]","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.632422Z","iopub.status.idle":"2023-10-21T03:47:25.632735Z","shell.execute_reply.started":"2023-10-21T03:47:25.632582Z","shell.execute_reply":"2023-10-21T03:47:25.632597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols:\n    print(f'Plots for {col}')\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1,3, 1)  \n    sns.boxplot(y=df_pvt[col], x=df_pvt['score'], color='#4082ed')\n    plt.title(\"Scatterplot with score\")\n\n    plt.subplot(1, 3, 2)  \n    sns.lineplot(y=df_pvt[col], x=df_pvt['score'], color='#40b9ed')\n    plt.title(\"trend with score\")\n\n    plt.subplot(1, 3, 3)  \n    sns.histplot(x=df_pvt[col], bins=50, kde=True, color='#40d3ed')\n    plt.title(f\"Histogram of {col}\")\n\n    plt.tight_layout() \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.633759Z","iopub.status.idle":"2023-10-21T03:47:25.634089Z","shell.execute_reply.started":"2023-10-21T03:47:25.633923Z","shell.execute_reply":"2023-10-21T03:47:25.633939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGB model tuned using Optuna","metadata":{}},{"cell_type":"code","source":"X = df_pvt.drop(['id','score'], axis = 1)\ny = df_pvt['score']","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.635067Z","iopub.status.idle":"2023-10-21T03:47:25.635414Z","shell.execute_reply.started":"2023-10-21T03:47:25.635253Z","shell.execute_reply":"2023-10-21T03:47:25.635269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.636752Z","iopub.status.idle":"2023-10-21T03:47:25.637059Z","shell.execute_reply.started":"2023-10-21T03:47:25.636901Z","shell.execute_reply":"2023-10-21T03:47:25.636916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef objective(trial):\n    params = {\n        'objective': 'reg:squarederror',  \n        'eval_metric': 'rmse',  \n        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)\n    }\n\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)\n\n    y_pred = model.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n\n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)  \n\nbest_params = study.best_params\nbest_rmse = study.best_value\n\nprint(f'Best Parameters: {best_params}')\nprint(f'Best RMSE: {best_rmse}')\n\nfinal_model = xgb.XGBRegressor(**best_params, random_state=42)\nfinal_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.638218Z","iopub.status.idle":"2023-10-21T03:47:25.638519Z","shell.execute_reply.started":"2023-10-21T03:47:25.638368Z","shell.execute_reply":"2023-10-21T03:47:25.638383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train')\nprint(mean_squared_error(y_train,final_model.predict(X_train)))\nprint('Test')\nprint(mean_squared_error(y_test,final_model.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.639711Z","iopub.status.idle":"2023-10-21T03:47:25.640041Z","shell.execute_reply.started":"2023-10-21T03:47:25.639875Z","shell.execute_reply":"2023-10-21T03:47:25.639891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = final_model.predict(test.drop('id',axis = 1))\ntest = test[['id']]\ntest['score'] = test_pred\n\ntest.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T03:47:25.641193Z","iopub.status.idle":"2023-10-21T03:47:25.641516Z","shell.execute_reply.started":"2023-10-21T03:47:25.641358Z","shell.execute_reply":"2023-10-21T03:47:25.641373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch.nn as nn\nimport torch\nfrom torch import optim\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-21T01:10:58.776544Z","iopub.execute_input":"2023-10-21T01:10:58.777489Z","iopub.status.idle":"2023-10-21T01:11:03.178451Z","shell.execute_reply.started":"2023-10-21T01:10:58.777455Z","shell.execute_reply":"2023-10-21T01:11:03.177368Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\n/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ***Competition Overview:***\n\n**Dataset:** The competition dataset comprises approximately 5000 logs of user inputs. These logs are generated while users are composing essays.\n**Task:** The goal of this competition is to predict the score that an essay received based on the log of user inputs. The scores are on a scale of 0 to 6, indicating the quality or effectiveness of the essay.\n**File and Field Information:**\nThe competition provides a CSV file called train_logs.csv, which contains the following fields:\n\n* id: This is a unique identifier for each essay.\n* event_id: An index indicating the order of events in the log, ordered chronologically.\n* down_time: The time when the down event (e.g., keypress or mouse click) occurred, measured in milliseconds.\n* up_time: The time when the up event (e.g., key release or mouse release) occurred, measured in milliseconds.\n* action_time: The duration of the event, which is the difference between down_time and up_time.\n* activity: The category of activity that the event belongs to. It can have values like \"Nonproduction,\" \"Input,\" \"Remove/Cut,\" \"Paste,\" \"Replace,\" or \"Move From [x1, y1] To [x2, y2]\".\n* down_event: The name of the event when the key or mouse is pressed.\n* up_event: The name of the event when the key or mouse is released.\n* text_change: The text that changed as a result of the event (if any). This field represents the alteration made to the essay text.\n* cursor_position: The character index of the text cursor after the event.\n* word_count: The word count of the essay after the event.\n\n**Objective:**\nParticipants in this competition are tasked with using the provided log data to build a predictive model. This model should take the log events as input and predict the essay's score on the 0 to 6 scale.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv')\ndf_score = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:03.180538Z","iopub.execute_input":"2023-10-21T01:11:03.181437Z","iopub.status.idle":"2023-10-21T01:11:18.656133Z","shell.execute_reply.started":"2023-10-21T01:11:03.181397Z","shell.execute_reply":"2023-10-21T01:11:18.655078Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:18.657355Z","iopub.execute_input":"2023-10-21T01:11:18.657642Z","iopub.status.idle":"2023-10-21T01:11:18.666543Z","shell.execute_reply.started":"2023-10-21T01:11:18.657617Z","shell.execute_reply":"2023-10-21T01:11:18.665633Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Activity \n* Nonproduction - The event does not alter the text in any way\n* Input - The event adds text to the essay\n* Remove/Cut - The event removes text from the essay\n* Paste - The event changes the text through a paste input\n* Replace - The event replaces a section of text with another string\n* Move From [x1, y1] To [x2, y2] - The event moves a section of text spanning character index x1, y1 to a new location x2, y2","metadata":{}},{"cell_type":"code","source":"def move_from(x):\n    if 'Move From' in x:\n        return 'Move From'\n    else:\n        return x ","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:18.668973Z","iopub.execute_input":"2023-10-21T01:11:18.669659Z","iopub.status.idle":"2023-10-21T01:11:18.676517Z","shell.execute_reply.started":"2023-10-21T01:11:18.669623Z","shell.execute_reply":"2023-10-21T01:11:18.675737Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.activity  = df['activity'].apply(lambda x : move_from(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:18.677624Z","iopub.execute_input":"2023-10-21T01:11:18.677921Z","iopub.status.idle":"2023-10-21T01:11:21.831401Z","shell.execute_reply.started":"2023-10-21T01:11:18.677896Z","shell.execute_reply":"2023-10-21T01:11:21.830576Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test.activity  = test['activity'].apply(lambda x : move_from(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:21.832518Z","iopub.execute_input":"2023-10-21T01:11:21.832887Z","iopub.status.idle":"2023-10-21T01:11:21.839031Z","shell.execute_reply.started":"2023-10-21T01:11:21.832852Z","shell.execute_reply":"2023-10-21T01:11:21.838065Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Creating Essay Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\n\nclass EssayDataset(Dataset):\n    def __init__(self, train_df, score_df):\n        self.train_df = train_df\n        self.score_df = score_df\n        self.unique_ids = train_df['id'].unique()\n        self.columns_to_return = ['action_time', 'activity', 'down_event', 'word_count']\n        self.down_event_to_index = {event: idx for idx, event in enumerate(unique_down_events)}\n        self.unidentified_de_index = self.down_event_to_index.get('Unidentified', len(unique_down_events))\n        self.activity_to_index =  {act: idx for idx, act in enumerate(unique_activities)}\n        self.unidentified_a_index = self.activity_to_index.get('Unidentified', len(unique_activities))\n        self.score_to_index =  {score: idx for idx, score in enumerate(unique_scores)}\n        \n\n\n    def __len__(self):\n        return len(self.unique_ids)\n\n    def __getitem__(self, idx):\n        essay_id = self.unique_ids[idx]\n        essay_data = self.train_df[self.train_df['id'] == essay_id]\n        essay_data = essay_data.sort_values(by='event_id')[self.columns_to_return]\n        \n        # Normalize and replace categories with indices\n        essay_data['action_time'] = (essay_data['action_time'] - essay_data['action_time'].min()) / (essay_data['action_time'].max() - essay_data['action_time'].min())\n        essay_data['word_count'] = (essay_data['word_count'] - essay_data['word_count'].min()) / (essay_data['word_count'].max() - essay_data['word_count'].min())\n        essay_data['down_event'] = essay_data['down_event'].map(self.down_event_to_index).fillna(self.unidentified_de_index).astype(int)\n        essay_data['activity'] = essay_data['activity'].map(self.activity_to_index).fillna(self.unidentified_a_index).astype(int)\n        \n        essay_score = self.score_to_index[self.score_df[self.score_df['id'] == essay_id]['score'].values[0]]\n        essay_tensor = torch.Tensor(essay_data.values)\n        return essay_tensor, essay_score","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:21.840326Z","iopub.execute_input":"2023-10-21T01:11:21.840589Z","iopub.status.idle":"2023-10-21T01:11:21.853521Z","shell.execute_reply.started":"2023-10-21T01:11:21.840565Z","shell.execute_reply":"2023-10-21T01:11:21.852450Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n\nunique_down_events = list(df['down_event'].unique())\nunique_activities = list(df['activity'].unique())\nunique_scores = list(df_score['score'].unique())\nprint(unique_scores)\nunique_activities.append('Unidentified')\n\ndataset = EssayDataset(df, df_score)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:21.854852Z","iopub.execute_input":"2023-10-21T01:11:21.855681Z","iopub.status.idle":"2023-10-21T01:11:23.754490Z","shell.execute_reply.started":"2023-10-21T01:11:21.855630Z","shell.execute_reply":"2023-10-21T01:11:23.753703Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[3.5, 6.0, 2.0, 4.0, 4.5, 2.5, 5.0, 3.0, 1.5, 5.5, 1.0, 0.5]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Input Embeddings\n","metadata":{}},{"cell_type":"code","source":"class InputEssayEmbeddings(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n\n        # Embedding for the categorical columns\n        self.embed_activity = nn.Embedding(len(unique_activities), (n_embed-2)//2)\n        self.embed_down_event = nn.Embedding(len(unique_down_events), (n_embed-2)//2)\n        \n        # activities: 6\n        # down_event: 131\n\n    def forward(self, input_tensor):\n        # Slice columns from the input tensor\n        action_time_col = input_tensor[:, :, 0:1]  # [B, num_events, 1]\n        activity_col = input_tensor[:, :, 1].long()  # [B, num_events]\n        down_event_col = input_tensor[:, :, 2].long()    # [B, num_events]\n        word_count_col = input_tensor[:, :, 3:4]  # [B, num_events, 1]\n\n        # Pass through respective embedding layers\n        embed_down_event = self.embed_down_event(down_event_col)  # [B, num_events, embed_dim_down_event]\n        embed_activity = self.embed_activity(activity_col)        # [B, num_events, embed_dim_activity]\n        \n        # Concatenate them with the rest of the tensor\n        out = torch.cat([action_time_col, embed_activity, embed_down_event, word_count_col], dim=2)\n        return out # [B, num_events, n_embed]\n    \nclass PositionalEncoding(nn.Module):\n    def __init__(self, n_embed, seq_len):\n        super().__init__()\n        # Dict size\n        self.emb = nn.Embedding(seq_len, n_embed)\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.device = torch.device(device)\n\n    def forward(self, x, batched=False):\n        \"\"\"\n        :param x: If using batching, should be [batch size, seq len, embedding dim]. Otherwise, [seq len, embedding dim]\n        :return: a tensor of the same size with positional embeddings added in\n        \"\"\"\n        # Second-to-last dimension will always be sequence length\n        input_size = x.shape[-2]\n        indices_to_embed = torch.tensor(np.asarray(range(0, input_size))).type(torch.LongTensor).to(self.device)\n        if batched:\n            # Use unsqueeze to form a [1, seq len, embedding dim] tensor -- broadcasting will ensure that this\n            # gets added correctly across the batch\n            emb_unsq = self.emb(indices_to_embed).unsqueeze(0)\n            return x + emb_unsq\n        else:\n            return x + self.emb(indices_to_embed)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:23.756883Z","iopub.execute_input":"2023-10-21T01:11:23.757164Z","iopub.status.idle":"2023-10-21T01:11:23.768658Z","shell.execute_reply.started":"2023-10-21T01:11:23.757139Z","shell.execute_reply":"2023-10-21T01:11:23.767720Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Define Architecture Sub Components","metadata":{}},{"cell_type":"code","source":"class FeedFoward(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embed, 4 * n_embed),\n            nn.ReLU(),\n            nn.Linear(4 * n_embed, n_embed),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n    \nclass Head(nn.Module):\n    def __init__(self, seq_length, n_embed, num_heads, n_internal):\n        super().__init__()\n        self.K = nn.Linear(n_embed, n_internal)\n        self.Q = nn.Linear(n_embed, n_internal)\n        self.V = nn.Linear(n_embed, n_internal)\n        self.w0 = nn.Linear(n_internal, n_embed // num_heads)\n        \n\n    def forward(self, input_vecs):\n        keys = self.K(input_vecs) # B, L, d_internal\n        d_k = keys.shape[-1]\n        queries = self.Q(input_vecs) # B, L, d_internal\n        value = self.V(input_vecs) # B, L, d_internal\n        \n        weights = torch.matmul(queries, keys.transpose(-2, -1)) * d_k**-0.5# L, L\n        attention = torch.softmax(weights, dim=-1)\n\n        logit = torch.matmul(attention , value) # B, L, d_internal\n        logit = self.w0(logit)\n        return logit\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, seq_length, n_embed, num_heads, n_internal):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(seq_length, n_embed, num_heads, n_internal) for _ in range(num_heads)])\n        \n        \n    def forward(self, input_vecs):\n        cls_tokens = []\n        for head in self.heads:\n            head_out = head(input_vecs)  \n            cls_tokens.append(head_out[:, 0])\n        cls_tokens_cat = torch.cat(cls_tokens, dim=-1)\n        #print(cls_tokens[0].shape)\n        return cls_tokens_cat.unsqueeze(1) # B, n_embed\n\nclass MHAConvolution(nn.Module):\n    def __init__(self, seq_length, n_embed, num_heads, n_internal, stride=1):\n        super().__init__()\n        self.stride = stride\n        self.window_size = seq_length\n        self.n_heads = num_heads\n        self.cls_token = nn.Parameter(torch.randn(1, 1, n_embed)) \n        self.pos_embedding = PositionalEncoding(n_embed, seq_length)\n        self.multi_head_attention = MultiHeadAttention( seq_length+1, n_embed, num_heads, n_internal)\n        self.ffwd = FeedFoward(n_embed)\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n\n    def forward(self, input_vecs, batched=False): # B, long_seq_len, n_embed\n        \n        outputs = []\n        #print(input_vecs.shape)\n        for i in range(0, input_vecs.size(1) - self.window_size + 1, self.stride): # TODO: think about ways to downsample other than stride\n            # prepend the cls token to the input\n            local_window = input_vecs[:, i:i+self.window_size, :]  # [B, seq_length, embed_size]\n            B, L, _ = local_window.size()\n            cls_tokens_repeated = self.cls_token.repeat(B, 1, 1) # B, 1, n_embed\n            local_window = self.pos_embedding(local_window, batched=batched)\n            local_window_cls = torch.cat([cls_tokens_repeated, local_window], dim=1)\n            attention_out = self.multi_head_attention(self.ln1(local_window_cls)) # B, 1, n_embed, a single cls vector cated from all heads\n            attention_out += cls_tokens_repeated # residual for the cls_tokens\n            out = attention_out + self.ffwd(self.ln2(attention_out))\n            out = out.view(B, -1) # B, n_heads*n_embed\n            outputs.append(out) \n        final = torch.stack(outputs, dim=1)\n        #print(final.shape)\n        return final # B, (long_seq_len-seq_len)/stride, n_embed\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:23.770118Z","iopub.execute_input":"2023-10-21T01:11:23.770433Z","iopub.status.idle":"2023-10-21T01:11:23.793234Z","shell.execute_reply.started":"2023-10-21T01:11:23.770407Z","shell.execute_reply":"2023-10-21T01:11:23.792397Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Define the Model","metadata":{}},{"cell_type":"code","source":"class GlobalAveragingTransformer(nn.Module):\n    def __init__(self, seq_len, n_embed, n_internal, num_heads, n_scores):\n        super().__init__()\n        self.seq_len = seq_len\n        self.mha_conv = MHAConvolution(seq_len, n_embed, num_heads, n_internal, stride=seq_len//2)\n        self.classifier = nn.Linear(n_embed, n_scores) # consider adding intermediat ffw layers\n        self.embedding = InputEssayEmbeddings(n_embed)\n        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n        \n    def forward(self, input_tensor, batched=False):\n        x = self.embedding(input_tensor)\n        x = self.mha_conv(x) # B, down_sampled_len, n_embed\n        x_permuted = x.permute(0, 2, 1) # B, n_embed, down_sampled_len\n        x = self.adaptive_pool(x_permuted).squeeze(-1) # B, n_embed\n        #print(x.shape)\n        x = self.classifier(x) # B, n_scores\n        if batched:\n            return x\n        else:\n            return x.squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:23.794250Z","iopub.execute_input":"2023-10-21T01:11:23.794508Z","iopub.status.idle":"2023-10-21T01:11:23.811294Z","shell.execute_reply.started":"2023-10-21T01:11:23.794486Z","shell.execute_reply":"2023-10-21T01:11:23.810384Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"model = GlobalAveragingTransformer(100, 16, 8, 4, len(unique_scores))\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device)\n\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:11:23.812350Z","iopub.execute_input":"2023-10-21T01:11:23.812588Z","iopub.status.idle":"2023-10-21T01:11:27.054449Z","shell.execute_reply.started":"2023-10-21T01:11:23.812566Z","shell.execute_reply":"2023-10-21T01:11:27.053518Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"GlobalAveragingTransformer(\n  (mha_conv): MHAConvolution(\n    (pos_embedding): PositionalEncoding(\n      (emb): Embedding(100, 16)\n    )\n    (multi_head_attention): MultiHeadAttention(\n      (heads): ModuleList(\n        (0-3): 4 x Head(\n          (K): Linear(in_features=16, out_features=8, bias=True)\n          (Q): Linear(in_features=16, out_features=8, bias=True)\n          (V): Linear(in_features=16, out_features=8, bias=True)\n          (w0): Linear(in_features=8, out_features=4, bias=True)\n        )\n      )\n    )\n    (ffwd): FeedFoward(\n      (net): Sequential(\n        (0): Linear(in_features=16, out_features=64, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=64, out_features=16, bias=True)\n      )\n    )\n    (ln1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n    (ln2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=16, out_features=12, bias=True)\n  (embedding): InputEssayEmbeddings(\n    (embed_activity): Embedding(7, 7)\n    (embed_down_event): Embedding(131, 7)\n  )\n  (adaptive_pool): AdaptiveAvgPool1d(output_size=1)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Run Training Loop","metadata":{}},{"cell_type":"code","source":"max_epochs = 80\nscheduler_enabled = False\n\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)\ncriterion = torch.nn.CrossEntropyLoss().to(device)\nmodel.train()\ndataset_size = len(train_loader)\nprint(\"Starting training loop\")\nprint(len(train_loader))\nfor epoch in range(max_epochs):\n    print(f\"Training epoch {str(epoch)}\")\n    num_correct = 0\n    num_samples = 0\n    i = 0\n    for batch_x, batch_y in train_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        optimizer.zero_grad()\n        logit = model.forward(batch_x, batched=True)\n        #print(logit.shape)\n        #print(batch_y.shape)\n        loss = criterion(logit, batch_y)\n        \n        #train_logger.add_scalar('loss', loss, epoch * dataset_size + i)\n        _, pred_labels = torch.max(logit, dim=1)\n        num_correct += (pred_labels == batch_y).sum().item()\n        num_samples += batch_y.size(0)\n        \n        loss.backward()\n        optimizer.step()\n        print(f\"finished loop {i} out of {dataset_size} with loss {loss}\")\n        i+=1\n        \n\n    acc = 100 * num_correct / num_samples\n    print(f\"training epoch {epoch} finished with acc {acc}\")\n    #train_logger.add_scalar('accuracy', acc, epoch * dataset_size + i - 1)\n    if scheduler_enabled:\n        scheduler.step(acc)\n        \n    model.eval()\n    num_correct = 0\n    num_samples = 0\n    with torch.no_grad():\n        for batch_x, batch_y in validation_data_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n            logits = model(batch_x)\n            _, predicted = torch.max(logits, 1)\n            num_samples += batch_y.size(0)\n            num_correct += (predicted == batch_y).sum().item()\n\n    acc = 100 * num_correct / num_samples\n    print(f\"validation epoch {epoch} finished with acc {acc}\")\n    #valid_logger.add_scalar('accuracy', acc, epoch * dataset_size + i - 1)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-21T01:35:44.666345Z","iopub.execute_input":"2023-10-21T01:35:44.667362Z","iopub.status.idle":"2023-10-21T01:36:12.273096Z","shell.execute_reply.started":"2023-10-21T01:35:44.667318Z","shell.execute_reply":"2023-10-21T01:36:12.271734Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Starting training loop\n1976\nTraining epoch 0\nfinished loop 0 out of 1976 with loss 2.876985549926758\nfinished loop 1 out of 1976 with loss 2.1154582500457764\nfinished loop 2 out of 1976 with loss 1.8051605224609375\nfinished loop 3 out of 1976 with loss 3.1828739643096924\nfinished loop 4 out of 1976 with loss 2.0638580322265625\nfinished loop 5 out of 1976 with loss 1.8022454977035522\nfinished loop 6 out of 1976 with loss 2.0413925647735596\nfinished loop 7 out of 1976 with loss 2.039069652557373\nfinished loop 8 out of 1976 with loss 1.7704156637191772\nfinished loop 9 out of 1976 with loss 1.7763490676879883\nfinished loop 10 out of 1976 with loss 3.6014316082000732\nfinished loop 11 out of 1976 with loss 2.0008387565612793\nfinished loop 12 out of 1976 with loss 2.1088063716888428\nfinished loop 13 out of 1976 with loss 2.0847132205963135\nfinished loop 14 out of 1976 with loss 3.5835838317871094\nfinished loop 15 out of 1976 with loss 1.7696325778961182\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m logit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(logit.shape)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(batch_y.shape)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logit, batch_y)\n","Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mGlobalAveragingTransformer.forward\u001b[0;34m(self, input_tensor, batched)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(input_tensor)\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, down_sampled_len, n_embed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     x_permuted \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# B, n_embed, down_sampled_len\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaptive_pool(x_permuted)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# B, n_embed\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[11], line 75\u001b[0m, in \u001b[0;36mMHAConvolution.forward\u001b[0;34m(self, input_vecs, batched)\u001b[0m\n\u001b[1;32m     73\u001b[0m local_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(local_window, batched\u001b[38;5;241m=\u001b[39mbatched)\n\u001b[1;32m     74\u001b[0m local_window_cls \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cls_tokens_repeated, local_window], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m attention_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_window_cls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B, 1, n_embed, a single cls vector cated from all heads\u001b[39;00m\n\u001b[1;32m     76\u001b[0m attention_out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cls_tokens_repeated \u001b[38;5;66;03m# residual for the cls_tokens\u001b[39;00m\n\u001b[1;32m     77\u001b[0m out \u001b[38;5;241m=\u001b[39m attention_out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(attention_out))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[11], line 45\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, input_vecs)\u001b[0m\n\u001b[1;32m     43\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads:\n\u001b[0;32m---> 45\u001b[0m     head_out \u001b[38;5;241m=\u001b[39m \u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_vecs\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     46\u001b[0m     cls_tokens\u001b[38;5;241m.\u001b[39mappend(head_out[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     47\u001b[0m cls_tokens_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(cls_tokens, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, input_vecs)\u001b[0m\n\u001b[1;32m     25\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ(input_vecs) \u001b[38;5;66;03m# B, L, d_internal\u001b[39;00m\n\u001b[1;32m     26\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV(input_vecs) \u001b[38;5;66;03m# B, L, d_internal\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m d_k\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;66;03m# L, L\u001b[39;00m\n\u001b[1;32m     29\u001b[0m attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m logit \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention , value) \u001b[38;5;66;03m# B, L, d_internal\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"df_agg = df.groupby(['id','activity']).agg({\n    'event_id': np.max,\n    'action_time' : [np.mean, np.sum, np.min, np.max],\n    'word_count' : np.max,\n    'cursor_position' : np.max\n}).reset_index()\ndf_agg.head(10)\n#df_agg.colum1ns = [i+\"_\"+j for i,j in df_agg.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:19:53.184823Z","iopub.execute_input":"2023-10-20T01:19:53.185099Z","iopub.status.idle":"2023-10-20T01:19:54.337995Z","shell.execute_reply.started":"2023-10-20T01:19:53.185080Z","shell.execute_reply":"2023-10-20T01:19:54.337226Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"         id       activity event_id action_time                    word_count  \\\n                               amax        mean     sum amin  amax       amax   \n0  001519c8          Input     2556  121.259204  243731   29   248        256   \n1  001519c8      Move From     2516    0.000000       0    0     0        256   \n2  001519c8  Nonproduction     2557  154.216667   18506    0  2259        256   \n3  001519c8     Remove/Cut     2555   81.846523   34130    2   529        256   \n4  001519c8        Replace     2482  125.142857     876   98   196        254   \n5  0022f953          Input     2359  122.750774  237891   22   250        323   \n6  0022f953  Nonproduction     2454   54.255906   13781    0  1758        321   \n7  0022f953          Paste     2241   71.000000      71   71    71        316   \n8  0022f953     Remove/Cut     2368   90.576923   23550   28   502        322   \n9  0022f953        Replace     2325   98.000000      98   98    98        320   \n\n  cursor_position  \n             amax  \n0            1495  \n1             466  \n2            1539  \n3            1482  \n4            1448  \n5            1613  \n6            1676  \n7             696  \n8            1597  \n9              80  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>id</th>\n      <th>activity</th>\n      <th>event_id</th>\n      <th colspan=\"4\" halign=\"left\">action_time</th>\n      <th>word_count</th>\n      <th>cursor_position</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>amax</th>\n      <th>mean</th>\n      <th>sum</th>\n      <th>amin</th>\n      <th>amax</th>\n      <th>amax</th>\n      <th>amax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001519c8</td>\n      <td>Input</td>\n      <td>2556</td>\n      <td>121.259204</td>\n      <td>243731</td>\n      <td>29</td>\n      <td>248</td>\n      <td>256</td>\n      <td>1495</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001519c8</td>\n      <td>Move From</td>\n      <td>2516</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>256</td>\n      <td>466</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001519c8</td>\n      <td>Nonproduction</td>\n      <td>2557</td>\n      <td>154.216667</td>\n      <td>18506</td>\n      <td>0</td>\n      <td>2259</td>\n      <td>256</td>\n      <td>1539</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001519c8</td>\n      <td>Remove/Cut</td>\n      <td>2555</td>\n      <td>81.846523</td>\n      <td>34130</td>\n      <td>2</td>\n      <td>529</td>\n      <td>256</td>\n      <td>1482</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>001519c8</td>\n      <td>Replace</td>\n      <td>2482</td>\n      <td>125.142857</td>\n      <td>876</td>\n      <td>98</td>\n      <td>196</td>\n      <td>254</td>\n      <td>1448</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0022f953</td>\n      <td>Input</td>\n      <td>2359</td>\n      <td>122.750774</td>\n      <td>237891</td>\n      <td>22</td>\n      <td>250</td>\n      <td>323</td>\n      <td>1613</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0022f953</td>\n      <td>Nonproduction</td>\n      <td>2454</td>\n      <td>54.255906</td>\n      <td>13781</td>\n      <td>0</td>\n      <td>1758</td>\n      <td>321</td>\n      <td>1676</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0022f953</td>\n      <td>Paste</td>\n      <td>2241</td>\n      <td>71.000000</td>\n      <td>71</td>\n      <td>71</td>\n      <td>71</td>\n      <td>316</td>\n      <td>696</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0022f953</td>\n      <td>Remove/Cut</td>\n      <td>2368</td>\n      <td>90.576923</td>\n      <td>23550</td>\n      <td>28</td>\n      <td>502</td>\n      <td>322</td>\n      <td>1597</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0022f953</td>\n      <td>Replace</td>\n      <td>2325</td>\n      <td>98.000000</td>\n      <td>98</td>\n      <td>98</td>\n      <td>98</td>\n      <td>320</td>\n      <td>80</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test = test.groupby(['id','activity']).agg({\n    'event_id': np.max,\n    'action_time' : [np.mean, np.sum, np.min, np.max],\n    'word_count' : np.max,\n    'cursor_position' : np.max\n}).reset_index()\n\ntest.columns = [i+\"_\"+j for i,j in test.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:09:32.26482Z","iopub.execute_input":"2023-10-03T14:09:32.265095Z","iopub.status.idle":"2023-10-03T14:09:32.278514Z","shell.execute_reply.started":"2023-10-03T14:09:32.265071Z","shell.execute_reply":"2023-10-03T14:09:32.277738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat = ['event_id_amax', 'action_time_mean',\n       'action_time_sum', 'action_time_amin', 'action_time_amax',\n       'word_count_amax', 'cursor_position_amax']\ndf_pvt = pd.pivot_table(df_agg, values =feat, index =['id_'],\n                         columns =['activity_'], aggfunc = np.max).reset_index()\n\ndf_pvt.columns = [i+\"_\"+j for i,j in df_pvt.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:21:51.766411Z","iopub.execute_input":"2023-10-03T14:21:51.766809Z","iopub.status.idle":"2023-10-03T14:21:51.800652Z","shell.execute_reply.started":"2023-10-03T14:21:51.766768Z","shell.execute_reply":"2023-10-03T14:21:51.799933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remaining_cols = list(set(feat) - set(test.columns))\nif len(remaining_cols) != 0:\n    for i in remaining_cols:\n        test[i] = 0","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:21:53.548206Z","iopub.execute_input":"2023-10-03T14:21:53.548834Z","iopub.status.idle":"2023-10-03T14:21:53.553055Z","shell.execute_reply.started":"2023-10-03T14:21:53.548796Z","shell.execute_reply":"2023-10-03T14:21:53.552295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.pivot_table(test, values = feat, index =['id_'],\n                         columns =['activity_'], aggfunc = np.max).reset_index()\n\ntest.columns = [i+\"_\"+j for i,j in test.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:21:57.0237Z","iopub.execute_input":"2023-10-03T14:21:57.024253Z","iopub.status.idle":"2023-10-03T14:21:57.037517Z","shell.execute_reply.started":"2023-10-03T14:21:57.024227Z","shell.execute_reply":"2023-10-03T14:21:57.03614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remaining_cols = list(set(df_pvt.columns) - set(test.columns))\nif len(remaining_cols) != 0:\n    for i in remaining_cols:\n        test[i] = 0","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:21:58.92086Z","iopub.execute_input":"2023-10-03T14:21:58.921225Z","iopub.status.idle":"2023-10-03T14:21:58.937191Z","shell.execute_reply.started":"2023-10-03T14:21:58.921197Z","shell.execute_reply":"2023-10-03T14:21:58.935988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[df_pvt.columns]","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:21:59.973409Z","iopub.execute_input":"2023-10-03T14:21:59.973795Z","iopub.status.idle":"2023-10-03T14:21:59.981097Z","shell.execute_reply.started":"2023-10-03T14:21:59.973765Z","shell.execute_reply":"2023-10-03T14:21:59.980002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[df_pvt.columns]\ndf_pvt = df_pvt.rename(columns={'id__':'id'})\ntest = test.rename(columns={'id__':'id'})\ndf_pvt = df_pvt.merge(df_score, on = 'id', how = 'left')","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:22:00.938179Z","iopub.execute_input":"2023-10-03T14:22:00.938516Z","iopub.status.idle":"2023-10-03T14:22:00.946017Z","shell.execute_reply.started":"2023-10-03T14:22:00.93849Z","shell.execute_reply":"2023-10-03T14:22:00.945011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:22:01.671406Z","iopub.execute_input":"2023-10-03T14:22:01.671783Z","iopub.status.idle":"2023-10-03T14:22:01.684362Z","shell.execute_reply.started":"2023-10-03T14:22:01.671729Z","shell.execute_reply":"2023-10-03T14:22:01.683177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:22:02.622346Z","iopub.execute_input":"2023-10-03T14:22:02.622976Z","iopub.status.idle":"2023-10-03T14:22:02.933893Z","shell.execute_reply.started":"2023-10-03T14:22:02.622944Z","shell.execute_reply":"2023-10-03T14:22:02.932806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(df_pvt['score'])\nplt.figure(figsize=(15, 7))\nsns.heatmap(df_pvt.drop(['id'],axis= 1).corr(), annot=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:22:04.038403Z","iopub.execute_input":"2023-10-03T14:22:04.039276Z","iopub.status.idle":"2023-10-03T14:22:05.01853Z","shell.execute_reply.started":"2023-10-03T14:22:04.039241Z","shell.execute_reply":"2023-10-03T14:22:05.017752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['action_time_amax_Input', 'action_time_amax_Move From',\n       'action_time_amax_Nonproduction', 'action_time_amax_Paste',\n       'action_time_amax_Remove/Cut', 'action_time_amax_Replace',\n       'action_time_amin_Input', 'action_time_amin_Move From',\n       'action_time_amin_Nonproduction', 'action_time_amin_Paste',\n       'action_time_amin_Remove/Cut', 'action_time_amin_Replace',\n       'action_time_mean_Input', 'action_time_mean_Move From',\n       'action_time_mean_Nonproduction', 'action_time_mean_Paste',\n       'action_time_mean_Remove/Cut', 'action_time_mean_Replace',\n       'action_time_sum_Input', 'action_time_sum_Move From',\n       'action_time_sum_Nonproduction', 'action_time_sum_Paste',\n       'action_time_sum_Remove/Cut', 'action_time_sum_Replace',\n       'cursor_position_amax_Input', 'cursor_position_amax_Move From',\n       'cursor_position_amax_Nonproduction', 'cursor_position_amax_Paste',\n       'cursor_position_amax_Remove/Cut', 'cursor_position_amax_Replace',\n       'event_id_amax_Input', 'event_id_amax_Move From',\n       'event_id_amax_Nonproduction', 'event_id_amax_Paste',\n       'event_id_amax_Remove/Cut', 'event_id_amax_Replace',\n       'word_count_amax_Input', 'word_count_amax_Move From',\n       'word_count_amax_Nonproduction', 'word_count_amax_Paste',\n       'word_count_amax_Remove/Cut', 'word_count_amax_Replace',]","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:22:05.019895Z","iopub.execute_input":"2023-10-03T14:22:05.02035Z","iopub.status.idle":"2023-10-03T14:22:05.026797Z","shell.execute_reply.started":"2023-10-03T14:22:05.020324Z","shell.execute_reply":"2023-10-03T14:22:05.025802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in cols:\n    print(f'Plots for {col}')\n    plt.figure(figsize=(15, 5))\n\n    plt.subplot(1,3, 1)  \n    sns.boxplot(y=df_pvt[col], x=df_pvt['score'], color='#4082ed')\n    plt.title(\"Scatterplot with score\")\n\n    plt.subplot(1, 3, 2)  \n    sns.lineplot(y=df_pvt[col], x=df_pvt['score'], color='#40b9ed')\n    plt.title(\"trend with score\")\n\n    plt.subplot(1, 3, 3)  \n    sns.histplot(x=df_pvt[col], bins=50, kde=True, color='#40d3ed')\n    plt.title(f\"Histogram of {col}\")\n\n    plt.tight_layout() \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:22:05.13486Z","iopub.execute_input":"2023-10-03T14:22:05.135584Z","iopub.status.idle":"2023-10-03T14:22:55.086369Z","shell.execute_reply.started":"2023-10-03T14:22:05.135544Z","shell.execute_reply":"2023-10-03T14:22:55.085148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGB model tuned using Optuna","metadata":{}},{"cell_type":"code","source":"X = df_pvt.drop(['id','score'], axis = 1)\ny = df_pvt['score']","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:45:08.742229Z","iopub.execute_input":"2023-10-03T14:45:08.742591Z","iopub.status.idle":"2023-10-03T14:45:08.749329Z","shell.execute_reply.started":"2023-10-03T14:45:08.742566Z","shell.execute_reply":"2023-10-03T14:45:08.747853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:45:09.777165Z","iopub.execute_input":"2023-10-03T14:45:09.777514Z","iopub.status.idle":"2023-10-03T14:45:09.782985Z","shell.execute_reply.started":"2023-10-03T14:45:09.777489Z","shell.execute_reply":"2023-10-03T14:45:09.781839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef objective(trial):\n    params = {\n        'objective': 'reg:squarederror',  \n        'eval_metric': 'rmse',  \n        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)\n    }\n\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)\n\n    y_pred = model.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n\n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)  \n\nbest_params = study.best_params\nbest_rmse = study.best_value\n\nprint(f'Best Parameters: {best_params}')\nprint(f'Best RMSE: {best_rmse}')\n\nfinal_model = xgb.XGBRegressor(**best_params, random_state=42)\nfinal_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:45:10.145259Z","iopub.execute_input":"2023-10-03T14:45:10.145634Z","iopub.status.idle":"2023-10-03T14:45:33.311984Z","shell.execute_reply.started":"2023-10-03T14:45:10.145605Z","shell.execute_reply":"2023-10-03T14:45:33.310977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train')\nprint(mean_squared_error(y_train,final_model.predict(X_train)))\nprint('Test')\nprint(mean_squared_error(y_test,final_model.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:45:33.316322Z","iopub.execute_input":"2023-10-03T14:45:33.317029Z","iopub.status.idle":"2023-10-03T14:45:33.432054Z","shell.execute_reply.started":"2023-10-03T14:45:33.316986Z","shell.execute_reply":"2023-10-03T14:45:33.431215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = final_model.predict(test.drop('id',axis = 1))\ntest = test[['id']]\ntest['score'] = test_pred\n\ntest.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:45:33.45262Z","iopub.execute_input":"2023-10-03T14:45:33.452957Z","iopub.status.idle":"2023-10-03T14:45:33.475165Z","shell.execute_reply.started":"2023-10-03T14:45:33.452931Z","shell.execute_reply":"2023-10-03T14:45:33.474376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MHA Transformer \n\n## Introduction\nWelcome to this Jupyter notebook developed for Stanford Ribonanza RNA Folding to create a model that predicts the structures of any RNA molecule using sequence to sequence NLP techniques.\n\n## Purpose\nThe primary purpose of this notebook is to:\n- Load and preprocess the competition data 📁\n- Engineer relevant features for model training 🏋️‍♂️\n- Train predictive models to make target variable predictions 🧠\n- Submit predictions to the competition environment 📤\n","metadata":{}},{"cell_type":"markdown","source":"## 📦 Import necessary libraries and modules\n","metadata":{}},{"cell_type":"code","source":"import torch  # 🔥 Import PyTorch for deep learning\nfrom torch.utils.data import random_split  # 📂 For splitting datasets\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd  # 🐼 Pandas for data manipulation\nfrom pathlib import Path  # 📁 pathlib for handling file paths\nimport numpy as np  # 🧮 NumPy for numerical operations\nfrom sklearn.preprocessing import OneHotEncoder  # 🧬 Scikit-Learn for one-hot encoding\nimport polars as pl  # 📊 Polars for data manipulation\nimport re  # 🧵 Regular expressions for text processing\nfrom tqdm import tqdm  # 🔄 tqdm for progress bar display\nimport torch.nn as nn\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:27:32.204896Z","iopub.execute_input":"2023-10-04T01:27:32.205255Z","iopub.status.idle":"2023-10-04T01:27:37.266611Z","shell.execute_reply.started":"2023-10-04T01:27:32.205220Z","shell.execute_reply":"2023-10-04T01:27:37.265449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 📁 Define file paths for dataset and output\n","metadata":{}},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/stanford-ribonanza-rna-folding/\")  # 📂 Directory for dataset\nTRAIN_CSV = DATA_DIR / \"train_data.csv\"  # 🚆 Training data in CSV format\nTRAIN_PARQUET_FILE = \"train_data.parquet\"  # 📦 Training data in Parquet format\nTEST_CSV = DATA_DIR / \"test_sequences.csv\"  # 🚀 Test sequences in CSV format\nTEST_PARQUET_FILE = \"test_sequences.parquet\"  # 📦 Test sequences in Parquet format\nPRED_CSV = \"submission.csv\"  # 📄 Output file for predictions\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:27:41.270804Z","iopub.execute_input":"2023-10-04T01:27:41.271300Z","iopub.status.idle":"2023-10-04T01:27:41.276407Z","shell.execute_reply.started":"2023-10-04T01:27:41.271272Z","shell.execute_reply":"2023-10-04T01:27:41.275683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_parquet(csv_file, parquet_file):\n    # 📊 Read CSV data using Polars\n    dummy_df = pl.scan_csv(csv_file)\n\n    # 🔍 Define a new schema mapping for specific columns\n    new_schema = {}\n    for key, value in dummy_df.schema.items():\n        if key.startswith(\"reactivity\"):\n            new_schema[key] = pl.Float32  # 📊 Convert 'reactivity' columns to Float32\n        else:\n            new_schema[key] = value\n\n    # 📊 Read CSV data with the new schema and write to Parquet\n    df = pl.scan_csv(csv_file, schema=new_schema)\n    \n    # 💾 Write data to Parquet format with specified settings\n    df.sink_parquet(\n        parquet_file,\n        compression='uncompressed',  # No compression for easy access\n        row_group_size=10,  # Adjust row group size as needed\n    )\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:27:41.959303Z","iopub.execute_input":"2023-10-04T01:27:41.960424Z","iopub.status.idle":"2023-10-04T01:27:41.968073Z","shell.execute_reply.started":"2023-10-04T01:27:41.960381Z","shell.execute_reply":"2023-10-04T01:27:41.967017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_parquet(TRAIN_CSV, TRAIN_PARQUET_FILE)  # 🚆 Training data\nto_parquet(TEST_CSV, TEST_PARQUET_FILE)    # 🚀 Test data\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:27:43.009860Z","iopub.execute_input":"2023-10-04T01:27:43.010194Z","iopub.status.idle":"2023-10-04T01:30:36.436630Z","shell.execute_reply.started":"2023-10-04T01:27:43.010170Z","shell.execute_reply":"2023-10-04T01:30:36.435900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass Indexer(object):\n    \"\"\"\n    Bijection between objects and integers starting at 0. Useful for mapping\n    labels, features, etc. into coordinates of a vector space.\n\n    Attributes:\n        objs_to_ints\n        ints_to_objs\n    \"\"\"\n    def __init__(self):\n        self.objs_to_ints = {}\n        self.ints_to_objs = {}\n\n    def __repr__(self):\n        return str([str(self.get_object(i)) for i in range(0, len(self))])\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __len__(self):\n        return len(self.objs_to_ints)\n\n    def get_object(self, index):\n        if (index not in self.ints_to_objs):\n            return None\n        else:\n            return self.ints_to_objs[index]\n\n    def contains(self, object):\n        return self.index_of(object) != -1\n\n    def index_of(self, object):\n        if (object not in self.objs_to_ints):\n            return -1\n        else:\n            return self.objs_to_ints[object]\n\n    def add_and_get_index(self, object, add=True):\n        if not add:\n            return self.index_of(object)\n        if (object not in self.objs_to_ints):\n            new_idx = len(self.objs_to_ints)\n            self.objs_to_ints[object] = new_idx\n            self.ints_to_objs[new_idx] = object\n        return self.objs_to_ints[object]\n\n\nclass SequenceDataset(Dataset):\n    def __init__(self, parquet_name, sequence_index, context_length=50):\n            \n        self.parquet_name = parquet_name\n        self.seq_indexer = sequence_index\n        self.df = pl.read_parquet(self.parquet_name)\n        self.df = self.df.filter(pl.col(\"SN_filter\") == 1.0)\n        # 🧬 Get reactivity column names using regular expression\n        reactivity_match = re.compile('(reactivity_[0-9])')\n        reactivity_names = [col for col in self.df.columns if reactivity_match.match(col)]\n        # 📊 Select only the reactivity columns\n        self.reactivity_df = self.df.select(reactivity_names)\n        # 📊 Select the 'sequence' column\n        self.sequence_df = self.df.select(\"sequence\")\n        self.context_length = context_length\n        \n    def __len__(self):\n        \"\"\"\n        Your code here\n        \"\"\"\n        return len(self.df)\n\n    def parse_row(self, idx):\n        sequence_row = self.sequence_df.row(idx)\n        reactivity_row = self.reactivity_df.row(idx)\n        # 🧬 Get the sequence string\n        sequence = list(sequence_row[0])\n        # 🧬 Encode the sequence array using the one-hot encoder\n        sequence_indices = torch.LongTensor(np.array([self.seq_indexer.index_of(s) for s in sequence]))\n        sequence_length = len(sequence)\n        start_idx = random.randint(0, len(sequence) - context_length)\n        selected_sequence = sequence_indices[start_idx : start_idx + context_length]\n        reactivity = torch.LongTensor(np.array(reactivity_row, dtype=np.float32)[start_idx : start_idx + context_length])\n        valid_mask = np.argwhere(~np.isnan(reactivity)).reshape(-1)\n        torch_valid_mask = torch.tensor(valid_mask, dtype=torch.long)\n        # 🧬 Replace nan values for reactivity with 0.0 TODO figure out how to smoth these, perhaps with information from other columns\n        reactivity = np.nan_to_num(reactivity, copy=False, nan=0.0)\n        \n        return selected_sequence, reactivity, valid_mask\n    \n    def __getitem__(self, idx):\n        # 📊 Get and parse data for the specified index\n        data = self.parse_row(idx)\n        return data\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:44:44.930968Z","iopub.execute_input":"2023-10-04T01:44:44.931418Z","iopub.status.idle":"2023-10-04T01:44:44.950341Z","shell.execute_reply.started":"2023-10-04T01:44:44.931389Z","shell.execute_reply":"2023-10-04T01:44:44.949191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 📚 Create the full training dataset and split it into training and validation datasets","metadata":{}},{"cell_type":"code","source":"vocab = ['A', 'G', 'U', 'C']\nvocab_index = Indexer()\nfor char in vocab:\n    vocab_index.add_and_get_index(char)\nprint(repr(vocab_index))\n\nfull_train_dataset = SequenceDataset(parquet_name=TRAIN_PARQUET_FILE, sequence_index=vocab_index, context_length=80)  # 🚆 Full training dataset\ngenerator1 = torch.Generator().manual_seed(42)  # 🌱 Initialize random seed generator\ntrain_dataset, val_dataset = random_split(full_train_dataset, [0.7, 0.3], generator1)  # 🎯 Split dataset into training (70%) and validation (30%)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:47:05.839064Z","iopub.execute_input":"2023-10-04T01:47:05.839517Z","iopub.status.idle":"2023-10-04T01:47:18.284575Z","shell.execute_reply.started":"2023-10-04T01:47:05.839480Z","shell.execute_reply":"2023-10-04T01:47:18.283288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🚂 Create data loaders for training and validation\n","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)  # 📦 Training data loader\nval_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0)  # 📦 Validation data loader\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:47:18.286662Z","iopub.execute_input":"2023-10-04T01:47:18.288037Z","iopub.status.idle":"2023-10-04T01:47:18.298276Z","shell.execute_reply.started":"2023-10-04T01:47:18.287991Z","shell.execute_reply":"2023-10-04T01:47:18.296719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 📉 Define loss functions for training and evaluation\n","metadata":{}},{"cell_type":"code","source":"# 📉 Define loss functions for training and evaluation\nimport torch.nn.functional as F\n\ndef loss_fn(output, target):\n    # 🪟 Clip the target values to be within the range [0, 1]\n    clipped_target = torch.clip(target, min=0, max=1)\n    # 📉 Calculate the mean squared error loss\n    mses = F.mse_loss(output, clipped_target, reduction='mean')\n    return mses\n\ndef mae_fn(output, target):\n    # 🪟 Clip the target values to be within the range [0, 1]\n    clipped_target = torch.clip(target, min=0, max=1)\n    # 📉 Calculate the mean absolute error loss\n    maes = F.l1_loss(output, clipped_target, reduction='mean')\n    return maes\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:45:14.714625Z","iopub.execute_input":"2023-10-04T01:45:14.715198Z","iopub.status.idle":"2023-10-04T01:45:14.722386Z","shell.execute_reply.started":"2023-10-04T01:45:14.715138Z","shell.execute_reply":"2023-10-04T01:45:14.721064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🧠 Create a neural network model using Multi Head Attention","metadata":{}},{"cell_type":"code","source":"class FeedFoward(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, seq_length, d_model, num_heads, d_internal):\n        super().__init__()\n        self.K = nn.Linear(d_model, d_internal)\n        self.Q = nn.Linear(d_model, d_internal)\n        self.V = nn.Linear(d_model, d_internal)\n        self.w0 = nn.Linear(d_internal, d_model // num_heads)\n        self.register_buffer('tril', torch.tril(torch.ones(seq_length, seq_length)))\n\n    def forward(self, input_vecs):\n        keys = self.K(input_vecs) # B, L, d_internal\n        d_k = keys.shape[-1]\n        queries = self.Q(input_vecs) # B, L, d_internal\n        value = self.V(input_vecs) # B, L, d_internal\n        weights = torch.matmul(queries, keys.transpose(-2, -1)) * d_k**-0.5# L, L\n        weights = weights.masked_fill(self.tril == 0, float('-inf'))\n        attention = torch.softmax(weights, dim=-1)\n\n        logit = torch.matmul(attention , value) # B, L, d_internal\n        logit = self.w0(logit)\n        return logit\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, seq_length, d_model, num_heads, d_internal):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(seq_length, d_model, num_heads, d_internal) for _ in range(num_heads)])\n        self.linear1 = nn.Linear(d_model, d_model)\n\n    def forward(self, input_vecs):\n        out = torch.cat([head(input_vecs) for head in self.heads], dim=-1)\n        out = self.linear1(out)\n        return out\n\nclass MHATransformerLayer(nn.Module):\n    def __init__(self, seq_length, d_model, num_heads, d_internal):\n        super().__init__()\n        self.multi_head_attention = MultiHeadAttention( seq_length, d_model, num_heads, d_internal)\n        self.ffwd = FeedFoward(d_model)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n    def forward(self, input_vecs):\n        x = self.multi_head_attention(self.ln1(input_vecs))\n        x += input_vecs\n        x = x + self.ffwd(self.ln2(x))\n\n        return x\n\nclass MHATransformer(nn.Module):\n    def __init__(self, vocab_size, num_positions, d_model, d_internal, num_layers, num_heads):\n    \n        super().__init__()\n        self.num_positions = num_positions\n        self.L = []\n        for ly in range(num_layers):\n            self.L.append(MHATransformerLayer(num_positions, d_model, num_heads, d_internal))\n        self.transformer_layers = nn.Sequential(*self.L)\n        self.classifier = nn.Linear(d_model, 1)\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.layer_norm = nn.LayerNorm(d_model)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, indices, batched=False):\n        logit = self.embedding(indices)\n        logit = self.transformer_layers(logit)\n        logit = self.classifier(logit)\n        logit = self.softmax(logit)\n        if batched:\n            return logit\n        else:\n            return logit.squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:45:16.705256Z","iopub.execute_input":"2023-10-04T01:45:16.705692Z","iopub.status.idle":"2023-10-04T01:45:16.728453Z","shell.execute_reply.started":"2023-10-04T01:45:16.705642Z","shell.execute_reply":"2023-10-04T01:45:16.727227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 🛠️ Set the device to GPU if available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncontext_length = 80\nmodel = MHATransformer(vocab_size=4, num_positions=context_length, d_model=64, d_internal=32, num_layers=1, num_heads=8).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:47:27.457830Z","iopub.execute_input":"2023-10-04T01:47:27.458266Z","iopub.status.idle":"2023-10-04T01:47:27.480395Z","shell.execute_reply.started":"2023-10-04T01:47:27.458233Z","shell.execute_reply":"2023-10-04T01:47:27.479411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n# Make sure we are using the GPU\ndevice\n#print(len(train_dataloader))\n#for  batch_x, batch_y, valid_mask in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n#    print(batch_x.shape)\n#    print(batch_y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:47:34.098638Z","iopub.execute_input":"2023-10-04T01:47:34.099066Z","iopub.status.idle":"2023-10-04T01:47:34.108750Z","shell.execute_reply.started":"2023-10-04T01:47:34.099036Z","shell.execute_reply":"2023-10-04T01:47:34.107053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 15\n\n# 📈 Define the optimizer with learning rate and weight decay\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=5e-4)\n\n# 🚂 Iterate over epochs\nfor epoch in range(n_epochs):\n    train_losses = []\n    train_maes = []\n    model.train()\n    \n    # 🚞 Iterate over batches in the training dataloader\n    for  batch_x, batch_y, valid_mask in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n        batch_x, batch_y, valid_mask = batch_x.to(device), batch_y.to(device), valid_mask.to(device)\n        optimizer.zero_grad()\n        out = model(batch_x, batched=True)\n        out = torch.squeeze(out)\n        loss = loss_fn(out[valid_mask], batch_y[valid_mask].float())\n        mae = mae_fn(out[valid_mask], batch_y[valid_mask])\n        loss.backward()\n        train_losses.append(loss.detach().cpu().numpy())\n        train_maes.append(mae.detach().cpu().numpy())\n        optimizer.step()\n        pbar.set_description(f\"Train loss {loss.detach().cpu().numpy():.4f}\")\n    \n    # 📊 Print average training loss and MAE for the epoch\n    print(f\"Epoch {epoch} train loss: \", np.mean(train_losses))\n    print(f\"Epoch {epoch} train mae: \", np.mean(train_maes))\n    \n    val_losses = []\n    val_maes = []\n    model.eval()\n    \n    # 🚞 Iterate over batches in the validation dataloader\n    for batch_x, batch_y, valid_mask in (pbar := tqdm(val_dataloader, position=0, leave=True)):\n        batch_x, batch_y, valid_mask = batch_x.to(device), batch_y.to(device), valid_mask.to(device)\n        optimizer.zero_grad()\n        out = model(batch_x, batched=True)\n        out = torch.squeeze(out)\n        loss = loss_fn(out[valid_mask], batch_y[valid_mask].float())\n        mae = mae_fn(out[valid_mask], batch_y[valid_mask])\n        val_losses.append(loss.detach().cpu().numpy())\n        val_maes.append(mae.detach().cpu().numpy())\n        pbar.set_description(f\"Validation loss {loss.detach().cpu().numpy():.4f}\")\n    \n    # 📊 Print average validation loss and MAE for the epoch\n    print(f\"Epoch {epoch} val loss: \", np.mean(val_losses))\n    print(f\"Epoch {epoch} val mae: \", np.mean(val_maes))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:50:31.571504Z","iopub.execute_input":"2023-10-04T01:50:31.571974Z","iopub.status.idle":"2023-10-04T02:03:10.436249Z","shell.execute_reply.started":"2023-10-04T01:50:31.571939Z","shell.execute_reply":"2023-10-04T02:03:10.434269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🗑️ Clear the GPU memory cache\n","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:36.282054Z","iopub.execute_input":"2023-09-30T20:26:36.282408Z","iopub.status.idle":"2023-09-30T20:26:36.420452Z","shell.execute_reply.started":"2023-09-30T20:26:36.282379Z","shell.execute_reply":"2023-09-30T20:26:36.419239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 📚 Define a custom dataset class for inference on a graph\n","metadata":{}},{"cell_type":"markdown","source":"**Explaination**: \n\n","metadata":{}},{"cell_type":"code","source":"class InferenceGraphDataset(Dataset):\n    def __init__(self, parquet_name, edge_distance=2, root=None, transform=None, pre_transform=None, pre_filter=None):\n        super().__init__(root, transform, pre_transform, pre_filter)\n        # 📄 Set the Parquet file name\n        self.parquet_name = parquet_name\n        # 📏 Set the edge distance for generating the adjacency matrix\n        self.edge_distance = edge_distance\n        # 🧮 Initialize the one-hot encoder for node features\n        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=4)\n        # 🧮 Fit the one-hot encoder to possible values (A, G, U, C)\n        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n        # 📊 Load the Parquet dataframe\n        self.df = pl.read_parquet(self.parquet_name)\n        # 📊 Select the 'sequence' and 'id_min' columns\n        self.sequence_df = self.df.select(\"sequence\")\n        self.id_min_df = self.df.select(\"id_min\")\n\n    def parse_row(self, idx):\n        # 📊 Read the row at the given index\n        sequence_row = self.sequence_df.row(idx)\n        id_min = self.id_min_df.row(idx)[0]\n\n        # 🧬 Get the sequence string and convert it to an array\n        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n        # 🧬 Encode the sequence array using the one-hot encoder\n        encoded_sequence = self.node_encoder.transform(sequence)\n        # 📏 Get the sequence length\n        sequence_length = len(sequence)\n        # 📊 Get the edge index using nearest adjacency function\n        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n        # 📏 Convert the edge index to a torch tensor\n        edge_index = torch.tensor(edges_np, dtype=torch.long)\n\n        # 📊 Define node features as the one-hot encoded sequence\n        node_features = torch.Tensor(encoded_sequence)\n        ids = torch.arange(id_min, id_min+sequence_length, 1)\n\n        data = Data(x=node_features, edge_index=edge_index, ids=ids)\n\n        return data\n\n    def len(self):\n        # 📏 Return the length of the dataset\n        return len(self.df)\n\n    def get(self, idx):\n        # 📊 Get and parse data for the specified index\n        data = self.parse_row(idx)\n        return data\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:38.411662Z","iopub.execute_input":"2023-09-30T20:26:38.412035Z","iopub.status.idle":"2023-09-30T20:26:38.421806Z","shell.execute_reply.started":"2023-09-30T20:26:38.412007Z","shell.execute_reply":"2023-09-30T20:26:38.420951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 📚 Create an inference dataset and dataloader","metadata":{}},{"cell_type":"code","source":"infer_dataset = InferenceGraphDataset(parquet_name=TEST_PARQUET_FILE, edge_distance=EDGE_DISTANCE)  # 🚀 Inference dataset\ninfer_dataloader = DataLoader(infer_dataset, batch_size=128, shuffle=False, num_workers=2)  # 📦 Inference dataloader\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:40.360240Z","iopub.execute_input":"2023-09-30T20:26:40.360575Z","iopub.status.idle":"2023-09-30T20:26:42.206202Z","shell.execute_reply.started":"2023-09-30T20:26:40.360549Z","shell.execute_reply":"2023-09-30T20:26:42.205194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🏭 Set the device to GPU if available, otherwise use CPU\n","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# 🏭 Move the model to the selected device (GPU or CPU) and switch to evaluation mode\nmodel = model.eval().to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:46.131062Z","iopub.execute_input":"2023-09-30T20:26:46.131398Z","iopub.status.idle":"2023-09-30T20:26:46.138151Z","shell.execute_reply.started":"2023-09-30T20:26:46.131370Z","shell.execute_reply":"2023-09-30T20:26:46.137135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🧮 Initialize empty arrays for IDs and predictions & Append IDs and predictions\n","metadata":{}},{"cell_type":"code","source":"ids = np.empty(shape=(0, 1), dtype=int)\npreds = np.empty(shape=(0, 1), dtype=np.float32)\n\n# 🚀 Iterate over batches in the inference dataloader\nfor batch in tqdm(infer_dataloader):\n    batch = batch.to(device)\n    out = model(batch.x, batch.edge_index).detach().cpu().numpy()\n\n    # 📦 Append IDs and predictions to the respective arrays\n    ids = np.append(ids, batch.ids.detach().cpu().numpy())\n    preds = np.append(preds, out)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:48.298832Z","iopub.execute_input":"2023-09-30T20:26:48.299212Z","iopub.status.idle":"2023-09-30T22:16:43.862235Z","shell.execute_reply.started":"2023-09-30T20:26:48.299184Z","shell.execute_reply":"2023-09-30T22:16:43.860996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 📊 Create a DataFrame for the submission\n","metadata":{}},{"cell_type":"code","source":"submission_df = pl.DataFrame({\"id\": ids, \"reactivity_DMS_MaP\": preds, \"reactivity_2A3_MaP\": preds})\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T22:17:27.749228Z","iopub.execute_input":"2023-09-30T22:17:27.749614Z","iopub.status.idle":"2023-09-30T22:17:30.957824Z","shell.execute_reply.started":"2023-09-30T22:17:27.749583Z","shell.execute_reply":"2023-09-30T22:17:30.956821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 💾 Write the submission DataFrame to a CSV file\n","metadata":{}},{"cell_type":"code","source":"submission_df.write_csv(PRED_CSV)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T22:17:48.424485Z","iopub.execute_input":"2023-09-30T22:17:48.424848Z","iopub.status.idle":"2023-09-30T22:35:34.221809Z","shell.execute_reply.started":"2023-09-30T22:17:48.424819Z","shell.execute_reply":"2023-09-30T22:35:34.220719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimpleGraphDataset(Dataset):\n    def __init__(self, parquet_name, edge_distance=5, root=None, transform=None, pre_transform=None, pre_filter=None):\n        super().__init__(root, transform, pre_transform, pre_filter)\n        # 📄 Set the Parquet file name\n        self.parquet_name = parquet_name\n        # 📏 Set the edge distance for generating the adjacency matrix\n        self.edge_distance = edge_distance\n        # 🧮 Initialize the one-hot encoder for node features\n        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=5)\n        # 🧮 Fit the one-hot encoder to possible values (A, G, U, C)\n        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n        # 📊 Load the Parquet dataframe\n        self.df = pl.read_parquet(self.parquet_name)\n        # 📊 Filter the dataframe by 'SN_filter' column where the value is 1.0\n        self.df = self.df.filter(pl.col(\"SN_filter\") == 1.0)\n        # 🧬 Get reactivity column names using regular expression\n        reactivity_match = re.compile('(reactivity_[0-9])')\n        reactivity_names = [col for col in self.df.columns if reactivity_match.match(col)]\n        # 📊 Select only the reactivity columns\n        self.reactivity_df = self.df.select(reactivity_names)\n        # 📊 Select the 'sequence' column\n        self.sequence_df = self.df.select(\"sequence\")\n        \n\n    def parse_row(self, idx):\n        # 📊 Read the row at the given index\n        sequence_row = self.sequence_df.row(idx)\n        reactivity_row = self.reactivity_df.row(idx)\n        # 🧬 Get the sequence string and convert it to an array\n        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n        # 🧬 Encode the sequence array using the one-hot encoder\n        encoded_sequence = self.node_encoder.transform(sequence)\n        # 📏 Get the sequence length\n        sequence_length = len(sequence)\n        # 📊 Get the edge index using nearest adjacency function\n        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n        # 📏 Convert the edge index to a torch tensor\n        edge_index = torch.tensor(edges_np, dtype=torch.long)\n        # 🧬 Get reactivity targets for nodes\n        reactivity = np.array(reactivity_row, dtype=np.float32)[0:sequence_length]\n        # 🔒 Create valid masks for nodes\n        valid_mask = np.argwhere(~np.isnan(reactivity)).reshape(-1)\n        torch_valid_mask = torch.tensor(valid_mask, dtype=torch.long)\n        # 🧬 Replace nan values for reactivity with 0.0 (not super important as they get masked)\n        reactivity = np.nan_to_num(reactivity, copy=False, nan=0.0)\n        # 📊 Define node features as the one-hot encoded sequence\n        node_features = torch.Tensor(encoded_sequence)\n        # 🎯 Define targets\n        targets = torch.Tensor(reactivity)\n        # 📊 Create a PyTorch Data object\n        data = Data(x=node_features, edge_index=edge_index, y=targets, valid_mask=torch_valid_mask)\n        return data\n\n    def len(self):\n        # 📏 Return the length of the dataset\n        return len(self.df)\n\n    def get(self, idx):\n        # 📊 Get and parse data for the specified index\n        data = self.parse_row(idx)\n        return data","metadata":{},"execution_count":null,"outputs":[]}]}
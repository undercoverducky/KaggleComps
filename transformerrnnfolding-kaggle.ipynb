{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Enhanced GNN baseline ğŸ¤–\n\n## Introduction ğŸŒŸ\nWelcome to this Jupyter notebook developed for Stanford Ribonanza RNA Folding to create a model that predicts the structures of any RNA molecule.\n\n\n### Inspiration and Credits ğŸ™Œ\nThis notebook is inspired by the work of fnands\n, available at [this Kaggle project](https://www.kaggle.com/code/fnands/a-quick-gnn-baseline/notebook). I extend my gratitude to fnands\n for sharing their insights and code.\n\nğŸŒŸ Explore my profile and other public projects, and don't forget to share your feedback! \nğŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainali) ğŸ‘ˆ\n\nğŸ™ Thank you for taking the time to review my work, and please give it a thumbs-up if you found it valuable! ğŸ‘\n\n## Purpose ğŸ¯\nThe primary purpose of this notebook is to:\n- Load and preprocess the competition data ğŸ“\n- Engineer relevant features for model training ğŸ‹ï¸â€â™‚ï¸\n- Train predictive models to make target variable predictions ğŸ§ \n- Submit predictions to the competition environment ğŸ“¤\n\n## Notebook Structure ğŸ“š\nThis notebook is structured as follows:\n1. **Data Preparation**: In this section, we load and preprocess the competition data.\n2. **Feature Engineering**: We generate and select relevant features for model training.\n3. **Model Training**: We train machine learning models on the prepared data.\n4. **Prediction and Submission**: We make predictions on the test data and submit them for evaluation.\n\n\n## How to Use ğŸ› ï¸\nTo use this notebook effectively, please follow these steps:\n1. Ensure you have the competition data and environment set up.\n2. Execute each cell sequentially to perform data preparation, feature engineering, model training, and prediction submission.\n3. Customize and adapt the code as needed to improve model performance or experiment with different approaches.\n\n**Note**: Make sure to replace any placeholder paths or configurations with your specific information.\n\n## Acknowledgments ğŸ™\nWe acknowledge theChild Mind Institute\n organizers for providing the dataset and the competition platform.\n\nLet's get started! Feel free to reach out if you have any questions or need assistance along the way.\nğŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainali) ğŸ‘ˆ","metadata":{}},{"cell_type":"code","source":"!pip install torch-geometric","metadata":{"execution":{"iopub.status.busy":"2023-10-01T01:23:24.639964Z","iopub.execute_input":"2023-10-01T01:23:24.640395Z","iopub.status.idle":"2023-10-01T01:23:50.834983Z","shell.execute_reply.started":"2023-10-01T01:23:24.640363Z","shell.execute_reply":"2023-10-01T01:23:50.833318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“¦ Import necessary libraries and modules\n","metadata":{}},{"cell_type":"code","source":"import torch  # ğŸ”¥ Import PyTorch for deep learning\nfrom torch.utils.data import random_split  # ğŸ“‚ For splitting datasets\nfrom torch_geometric.data import Data, Dataset  # ğŸ“Š PyTorch Geometric for graph data handling\nfrom torch_geometric.loader import DataLoader  # ğŸšš DataLoader for batching data\nimport pandas as pd  # ğŸ¼ Pandas for data manipulation\nfrom pathlib import Path  # ğŸ“ pathlib for handling file paths\nimport numpy as np  # ğŸ§® NumPy for numerical operations\nfrom sklearn.preprocessing import OneHotEncoder  # ğŸ§¬ Scikit-Learn for one-hot encoding\nimport polars as pl  # ğŸ“Š Polars for data manipulation\nimport re  # ğŸ§µ Regular expressions for text processing\nfrom tqdm import tqdm  # ğŸ”„ tqdm for progress bar display\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T01:23:50.837646Z","iopub.execute_input":"2023-10-01T01:23:50.838078Z","iopub.status.idle":"2023-10-01T01:23:57.255061Z","shell.execute_reply.started":"2023-10-01T01:23:50.838021Z","shell.execute_reply":"2023-10-01T01:23:57.253997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“ Define file paths for dataset and output\n","metadata":{}},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/stanford-ribonanza-rna-folding/\")  # ğŸ“‚ Directory for dataset\nTRAIN_CSV = DATA_DIR / \"train_data.csv\"  # ğŸš† Training data in CSV format\nTRAIN_PARQUET_FILE = \"train_data.parquet\"  # ğŸ“¦ Training data in Parquet format\nTEST_CSV = DATA_DIR / \"test_sequences.csv\"  # ğŸš€ Test sequences in CSV format\nTEST_PARQUET_FILE = \"test_sequences.parquet\"  # ğŸ“¦ Test sequences in Parquet format\nPRED_CSV = \"submission.csv\"  # ğŸ“„ Output file for predictions\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T01:23:20.432067Z","iopub.execute_input":"2023-10-01T01:23:20.432862Z","iopub.status.idle":"2023-10-01T01:23:20.723876Z","shell.execute_reply.started":"2023-10-01T01:23:20.432825Z","shell.execute_reply":"2023-10-01T01:23:20.722326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“ Define a function to convert CSV to Parquet","metadata":{}},{"cell_type":"markdown","source":"**Explaination**:\n\nThis code defines a Python function named `to_parquet` that takes two parameters: `csv_file` and `parquet_file`. The purpose of this function is to read data from a CSV file, manipulate the schema of the data, and then save it as a Parquet file with specific settings.\n\n1. `dummy_df = pl.scan_csv(csv_file)`: This line reads the CSV data from the `csv_file` using the Polars library and stores it in a DataFrame called `dummy_df`. This DataFrame is essentially a table that holds the CSV data.\n\n2. `new_schema = {}`: This line initializes an empty dictionary called `new_schema`. This dictionary will be used to define a new schema for the DataFrame with specific column types.\n\n3. `for key, value in dummy_df.schema.items():`: This line starts a loop that iterates over the columns of the `dummy_df` DataFrame and their corresponding data types in the schema.\n\n4. `if key.startswith(\"reactivity\"):`: Within the loop, this line checks if the column name (`key`) starts with the string \"reactivity.\" If it does, it means that this column is related to reactivity, and we want to change its data type.\n\n5. `new_schema[key] = pl.Float32`: If the column name starts with \"reactivity,\" this line updates the `new_schema` dictionary to set the data type of that column to `Float32`. This conversion is done because Parquet requires specific data types for its columns.\n\n6. `else:`: If the column name does not start with \"reactivity,\" this line executes when dealing with columns other than those related to reactivity.\n\n7. `new_schema[key] = value`: For columns that are not related to reactivity, this line simply copies the existing data type from the original schema to the `new_schema` dictionary.\n\n8. `df = pl.scan_csv(csv_file, schema=new_schema)`: After defining the new schema, this line reads the CSV data from `csv_file` again but uses the `new_schema` to define the data types for the columns. The resulting DataFrame `df` now has the desired schema.\n\n9. `df.sink_parquet(...)`: This line writes the data from the DataFrame `df` to a Parquet file specified by `parquet_file`. It includes some additional settings:\n   - `compression='uncompressed'`: It specifies that the Parquet file should not be compressed, which makes it easy to access but may result in a larger file size.\n   - `row_group_size=10`: It sets the row group size for the Parquet file to 10. Row groups are a way to divide the data in a Parquet file, and this parameter allows you to control the size of those groups.\n\nThis code reads a CSV file, modifies the schema for specific columns, and then saves the data to a Parquet file with customizable settings.\n","metadata":{}},{"cell_type":"code","source":"def to_parquet(csv_file, parquet_file):\n    # ğŸ“Š Read CSV data using Polars\n    dummy_df = pl.scan_csv(csv_file)\n\n    # ğŸ” Define a new schema mapping for specific columns\n    new_schema = {}\n    for key, value in dummy_df.schema.items():\n        if key.startswith(\"reactivity\"):\n            new_schema[key] = pl.Float32  # ğŸ“Š Convert 'reactivity' columns to Float32\n        else:\n            new_schema[key] = value\n\n    # ğŸ“Š Read CSV data with the new schema and write to Parquet\n    df = pl.scan_csv(csv_file, schema=new_schema)\n    \n    # ğŸ’¾ Write data to Parquet format with specified settings\n    df.sink_parquet(\n        parquet_file,\n        compression='uncompressed',  # No compression for easy access\n        row_group_size=10,  # Adjust row group size as needed\n    )\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:26:00.789863Z","iopub.execute_input":"2023-09-30T19:26:00.790513Z","iopub.status.idle":"2023-09-30T19:26:00.796020Z","shell.execute_reply.started":"2023-09-30T19:26:00.790486Z","shell.execute_reply":"2023-09-30T19:26:00.795141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“ Convert training and test CSV data to Parquet format\n","metadata":{}},{"cell_type":"code","source":"to_parquet(TRAIN_CSV, TRAIN_PARQUET_FILE)  # ğŸš† Training data\nto_parquet(TEST_CSV, TEST_PARQUET_FILE)    # ğŸš€ Test data\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:26:08.030341Z","iopub.execute_input":"2023-09-30T19:26:08.030676Z","iopub.status.idle":"2023-09-30T19:28:49.762942Z","shell.execute_reply.started":"2023-09-30T19:26:08.030652Z","shell.execute_reply":"2023-09-30T19:28:49.761728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“ Define a function to generate nearest adjacency matrix\n","metadata":{}},{"cell_type":"markdown","source":"**Explaination**:\n\nThis code defines a Python function called `nearest_adjacency` that generates adjacency information for elements in a sequence. It calculates the neighbors of each element within a given range, considering both positive and negative offsets. The code includes support for circular connections if `loops` is set to `True`. \n\n1. `base = np.arange(sequence_length)`: This line creates a NumPy array `base` containing integers from 0 to `sequence_length-1`. This array represents the sequence of elements for which adjacency is being calculated.\n\n2. `connections = []`: This initializes an empty list `connections` where the adjacency information will be stored.\n\n3. `for i in range(-n, n + 1):`: This starts a loop that iterates through a range of values from `-n` to `n`, inclusive. This loop considers neighbors within a specified range around each element.\n\n4. `if i == 0 and not loops:`: This checks if `i` is equal to 0 (indicating the current element itself) and if `loops` is set to `False`. If both conditions are met, it continues to the next iteration of the loop, skipping the case where an element is considered its own neighbor.\n\n5. `elif i == 0 and loops:`: This handles the case where `i` is equal to 0, but `loops` is set to `True`. In this case, it creates circular connections by stacking the `base` array on top of itself. This ensures that each element is connected to itself.\n\n6. `neighbours = base.take(range(i, sequence_length + i), mode='wrap')`: For non-zero values of `i`, this line calculates the neighbors of each element. It uses the `take` method to index the `base` array with values wrapped around using the `mode='wrap'` parameter. This creates circular connections for positive and negative offsets.\n\n7. `stack = np.vstack([base, neighbours])`: This stacks the `base` array and the `neighbours` array vertically to create a matrix where each row represents an element and its neighbors.\n\n8. The `if` and `elif` blocks handle separating connections for positive and negative offsets. If `i` is negative, it appends the connections starting from the `i`-th column to the end of the matrix. If `i` is positive, it appends the connections from the beginning of the matrix up to the `i`-th column.\n\n9. `connections.append(stack)`: This appends the `stack` matrix, representing the adjacency information for a specific offset `i`, to the `connections` list.\n\n10. After the loop, the code combines all the adjacency matrices in the `connections` list horizontally using `np.hstack(connections)` and returns the result. This matrix represents the combined adjacency information for all elements in the sequence, considering the specified range and circular connections as needed.\n\nIn summary, this code generates adjacency information for a sequence of elements, allowing for circular connections and customizable neighbor ranges. It returns a matrix where each row corresponds to an element, and columns represent its neighbors within the specified range.","metadata":{}},{"cell_type":"code","source":"def nearest_adjacency(sequence_length, n=2, loops=True):\n    base = np.arange(sequence_length)\n    connections = []\n\n    for i in range(-n, n + 1):\n        if i == 0 and not loops:\n            continue\n        elif i == 0 and loops:\n            stack = np.vstack([base, base])\n            connections.append(stack)\n            continue\n        \n        # ğŸ”„ Wrap around the sequence for circular connections\n        neighbours = base.take(range(i, sequence_length + i), mode='wrap')\n        stack = np.vstack([base, neighbours])\n\n        # Separate connections for positive and negative offsets\n        if i < 0:\n            connections.append(stack[:, -i:])\n        elif i > 0:\n            connections.append(stack[:, :-i])\n\n    # Combine connections horizontally\n    return np.hstack(connections)\nprint(nearest_adjacency(10, n=1, loops=False))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:50:14.063746Z","iopub.execute_input":"2023-09-30T19:50:14.064120Z","iopub.status.idle":"2023-09-30T19:50:14.072574Z","shell.execute_reply.started":"2023-09-30T19:50:14.064089Z","shell.execute_reply":"2023-09-30T19:50:14.071625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“ Define the edge distance for generating adjacency matrix","metadata":{}},{"cell_type":"code","source":"EDGE_DISTANCE = 4 #Edge distance for generating adjacency matrix.\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:31:30.450915Z","iopub.execute_input":"2023-09-30T19:31:30.451280Z","iopub.status.idle":"2023-09-30T19:31:30.456058Z","shell.execute_reply.started":"2023-09-30T19:31:30.451252Z","shell.execute_reply":"2023-09-30T19:31:30.454848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“Š Define a custom dataset class for a simple graph dataset","metadata":{}},{"cell_type":"markdown","source":"**Explaination**:\n\nThis code defines a PyTorch dataset class called `SimpleGraphDataset` for working with graph data where nodes have one-hot encoded sequence information and target values. It reads data from a Parquet file, processes it, and provides a way to access individual data samples.\n\n1. `class SimpleGraphDataset(Dataset):`: This line defines a class `SimpleGraphDataset` that inherits from PyTorch's `Dataset` class, indicating that this class will be used to create a custom dataset for PyTorch.\n\n2. `def __init__(self, parquet_name, edge_distance=5, root=None, transform=None, pre_transform=None, pre_filter=None):`: This is the constructor method for the class. It initializes the dataset object and accepts several parameters:\n   - `parquet_name`: The name of the Parquet file containing the data.\n   - `edge_distance`: The distance for generating the adjacency matrix.\n   - `root`, `transform`, `pre_transform`, `pre_filter`: Parameters inherited from the `Dataset` class for custom transformations and filtering (optional).\n\n3. `super().__init__(root, transform, pre_transform, pre_filter)`: This line calls the constructor of the parent class (`Dataset`) to initialize the dataset with any provided parameters.\n\n4. `self.parquet_name = parquet_name`: It sets the name of the Parquet file as an instance variable.\n\n5. `self.edge_distance = edge_distance`: It sets the edge distance for generating the adjacency matrix as an instance variable.\n\n6. `self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=5)`: This line initializes a one-hot encoder (`node_encoder`) with specific settings:\n   - `sparse_output=False`: Specifies that the output should not be sparse.\n   - `max_categories=5`: Sets the maximum number of categories to 5.\n\n7. `self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))`: It fits the one-hot encoder to the possible values ('A', 'G', 'U', 'C') by reshaping them into a column vector.\n\n8. `self.df = pl.read_parquet(self.parquet_name)`: This line reads the data from the Parquet file using Polars (`pl`) and stores it in the `self.df` DataFrame.\n\n9. `self.df = self.df.filter(pl.col(\"SN_filter\") == 1.0)`: It filters the DataFrame to keep only rows where the \"SN_filter\" column has a value of 1.0.\n\n10. The code uses regular expressions to identify and select columns with names matching the pattern \"reactivity_[0-9].\" These columns are assumed to contain reactivity information.\n\n11. `self.reactivity_df = self.df.select(reactivity_names)`: It selects only the columns related to reactivity and stores them in the `self.reactivity_df` DataFrame.\n\n12. `self.sequence_df = self.df.select(\"sequence\")`: This line selects the \"sequence\" column from the DataFrame and stores it in the `self.sequence_df` DataFrame.\n\n13. `def parse_row(self, idx):`: This method is used to parse a row from the dataset. It takes an index `idx` as input and returns a PyTorch `Data` object containing node features, adjacency information, targets, and valid masks.\n\n14. The `parse_row` method reads the sequence and reactivity information for the given index and performs the following steps:\n   - It converts the sequence string into a one-hot encoded array.\n   - Calculates the adjacency matrix using the `nearest_adjacency` function.\n   - Processes reactivity information and creates a valid mask.\n   - Defines node features, targets, and creates a PyTorch `Data` object.\n\n15. `def len(self):`: This method returns the length of the dataset, which is the number of rows in the DataFrame.\n\n16. `def get(self, idx):`: This method is used to retrieve a data sample at a specified index. It calls the `parse_row` method to parse the data and returns a PyTorch `Data` object for the specified index.\n\nThis class provides a convenient way to work with graph data stored in a Parquet file, handling data loading, preprocessing, and providing access to individual data samples as PyTorch tensors.","metadata":{}},{"cell_type":"code","source":"class SimpleGraphDataset(Dataset):\n    def __init__(self, parquet_name, edge_distance=5, root=None, transform=None, pre_transform=None, pre_filter=None):\n        super().__init__(root, transform, pre_transform, pre_filter)\n        # ğŸ“„ Set the Parquet file name\n        self.parquet_name = parquet_name\n        # ğŸ“ Set the edge distance for generating the adjacency matrix\n        self.edge_distance = edge_distance\n        # ğŸ§® Initialize the one-hot encoder for node features\n        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=5)\n        # ğŸ§® Fit the one-hot encoder to possible values (A, G, U, C)\n        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n        # ğŸ“Š Load the Parquet dataframe\n        self.df = pl.read_parquet(self.parquet_name)\n        # ğŸ“Š Filter the dataframe by 'SN_filter' column where the value is 1.0\n        self.df = self.df.filter(pl.col(\"SN_filter\") == 1.0)\n        # ğŸ§¬ Get reactivity column names using regular expression\n        reactivity_match = re.compile('(reactivity_[0-9])')\n        reactivity_names = [col for col in self.df.columns if reactivity_match.match(col)]\n        # ğŸ“Š Select only the reactivity columns\n        self.reactivity_df = self.df.select(reactivity_names)\n        # ğŸ“Š Select the 'sequence' column\n        self.sequence_df = self.df.select(\"sequence\")\n\n    def parse_row(self, idx):\n        # ğŸ“Š Read the row at the given index\n        sequence_row = self.sequence_df.row(idx)\n        reactivity_row = self.reactivity_df.row(idx)\n        # ğŸ§¬ Get the sequence string and convert it to an array\n        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n        # ğŸ§¬ Encode the sequence array using the one-hot encoder\n        encoded_sequence = self.node_encoder.transform(sequence)\n        # ğŸ“ Get the sequence length\n        sequence_length = len(sequence)\n        # ğŸ“Š Get the edge index using nearest adjacency function\n        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n        # ğŸ“ Convert the edge index to a torch tensor\n        edge_index = torch.tensor(edges_np, dtype=torch.long)\n        # ğŸ§¬ Get reactivity targets for nodes\n        reactivity = np.array(reactivity_row, dtype=np.float32)[0:sequence_length]\n        # ğŸ”’ Create valid masks for nodes\n        valid_mask = np.argwhere(~np.isnan(reactivity)).reshape(-1)\n        torch_valid_mask = torch.tensor(valid_mask, dtype=torch.long)\n        # ğŸ§¬ Replace nan values for reactivity with 0.0 (not super important as they get masked)\n        reactivity = np.nan_to_num(reactivity, copy=False, nan=0.0)\n        # ğŸ“Š Define node features as the one-hot encoded sequence\n        node_features = torch.Tensor(encoded_sequence)\n        # ğŸ¯ Define targets\n        targets = torch.Tensor(reactivity)\n        # ğŸ“Š Create a PyTorch Data object\n        data = Data(x=node_features, edge_index=edge_index, y=targets, valid_mask=torch_valid_mask)\n        return data\n\n    def len(self):\n        # ğŸ“ Return the length of the dataset\n        return len(self.df)\n\n    def get(self, idx):\n        # ğŸ“Š Get and parse data for the specified index\n        data = self.parse_row(idx)\n        return data\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:49:20.776837Z","iopub.execute_input":"2023-09-30T19:49:20.777369Z","iopub.status.idle":"2023-09-30T19:49:20.789635Z","shell.execute_reply.started":"2023-09-30T19:49:20.777333Z","shell.execute_reply":"2023-09-30T19:49:20.788522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“š Create the full training dataset and split it into training and validation datasets","metadata":{}},{"cell_type":"code","source":"full_train_dataset = SimpleGraphDataset(parquet_name=TRAIN_PARQUET_FILE, edge_distance=EDGE_DISTANCE)  # ğŸš† Full training dataset\ngenerator1 = torch.Generator().manual_seed(42)  # ğŸŒ± Initialize random seed generator\ntrain_dataset, val_dataset = random_split(full_train_dataset, [0.7, 0.3], generator1)  # ğŸ¯ Split dataset into training (70%) and validation (30%)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:49:30.609064Z","iopub.execute_input":"2023-09-30T19:49:30.609421Z","iopub.status.idle":"2023-09-30T19:49:49.120544Z","shell.execute_reply.started":"2023-09-30T19:49:30.609395Z","shell.execute_reply":"2023-09-30T19:49:49.119531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸš‚ Create data loaders for training and validation\n","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)  # ğŸ“¦ Training data loader\nval_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2)  # ğŸ“¦ Validation data loader\nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:49:53.750097Z","iopub.execute_input":"2023-09-30T19:49:53.750483Z","iopub.status.idle":"2023-09-30T19:49:53.758911Z","shell.execute_reply.started":"2023-09-30T19:49:53.750453Z","shell.execute_reply":"2023-09-30T19:49:53.757979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“‰ Define loss functions for training and evaluation\n","metadata":{}},{"cell_type":"code","source":"# ğŸ“‰ Define loss functions for training and evaluation\nimport torch.nn.functional as F\n\ndef loss_fn(output, target):\n    # ğŸªŸ Clip the target values to be within the range [0, 1]\n    clipped_target = torch.clip(target, min=0, max=1)\n    # ğŸ“‰ Calculate the mean squared error loss\n    mses = F.mse_loss(output, clipped_target, reduction='mean')\n    return mses\n\ndef mae_fn(output, target):\n    # ğŸªŸ Clip the target values to be within the range [0, 1]\n    clipped_target = torch.clip(target, min=0, max=1)\n    # ğŸ“‰ Calculate the mean absolute error loss\n    maes = F.l1_loss(output, clipped_target, reduction='mean')\n    return maes\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:49:55.767516Z","iopub.execute_input":"2023-09-30T19:49:55.767866Z","iopub.status.idle":"2023-09-30T19:49:55.774015Z","shell.execute_reply.started":"2023-09-30T19:49:55.767836Z","shell.execute_reply":"2023-09-30T19:49:55.772973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ§  Create a neural network model using EdgeCNN","metadata":{}},{"cell_type":"code","source":"from torch_geometric.nn.models import EdgeCNN\n\n# ğŸ› ï¸ Set the device to GPU if available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# ğŸ—ï¸ Initialize the EdgeCNN model with specified parameters\nmodel = EdgeCNN(\n    in_channels=full_train_dataset.num_features,  # ğŸ“Š Input features determined by the dataset\n    hidden_channels=128,  # ğŸ•³ï¸ Number of hidden channels in the model\n    num_layers=4,  # ğŸ§± Number of layers in the model\n    out_channels=1  # ğŸ“¤ Number of output channels\n).to(device)  # ğŸ—ï¸ Move the model to the selected device (GPU or CPU)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:50:20.936580Z","iopub.execute_input":"2023-09-30T19:50:20.936957Z","iopub.status.idle":"2023-09-30T19:50:20.953458Z","shell.execute_reply.started":"2023-09-30T19:50:20.936923Z","shell.execute_reply":"2023-09-30T19:50:20.952623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure we are using the GPU\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:50:47.723247Z","iopub.execute_input":"2023-09-30T19:50:47.723592Z","iopub.status.idle":"2023-09-30T19:50:47.729968Z","shell.execute_reply.started":"2023-09-30T19:50:47.723566Z","shell.execute_reply":"2023-09-30T19:50:47.728836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ”„ Training loop\n","metadata":{}},{"cell_type":"markdown","source":"**Explaination**:\n\nThis code is a snippet for training a neural network model using PyTorch for some task, such as regression, classification, or another machine learning problem. It follows a typical training loop structure and monitors training and validation loss and mean absolute error (MAE) for each epoch. \n\n1. `n_epochs = 15`: This line defines the number of training epochs, which is the number of times the entire training dataset will be processed by the model during training.\n\n2. `optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=5e-4)`: Here, an Adam optimizer is defined with the following settings:\n   - `model.parameters()`: It specifies the model's parameters that the optimizer will update during training.\n   - `lr=0.0003`: This sets the learning rate, controlling the step size during optimization.\n   - `weight_decay=5e-4`: Weight decay is a regularization technique to prevent overfitting by penalizing large weights.\n\n3. `for epoch in range(n_epochs):`: This loop iterates over the specified number of training epochs.\n\n4. `train_losses = []` and `train_maes = []`: These lines initialize empty lists to store training losses and mean absolute errors for each batch in the training dataset.\n\n5. `model.train()`: This sets the model in training mode, which can be important for certain layers or operations that behave differently during training and inference (e.g., dropout, batch normalization).\n\n6. The next inner loop iterates over batches in the `train_dataloader` using the `tqdm` progress bar for visualization. The code within this loop performs the following steps for each batch:\n\n   - `batch = batch.to(device)`: It moves the batch of data to the specified computing device (e.g., CPU or GPU) to utilize hardware acceleration.\n\n   - `optimizer.zero_grad()`: This resets the gradients of the model's parameters to zero before computing gradients for the current batch.\n\n   - `out = model(batch.x, batch.edge_index)`: It passes the input data (`batch.x` and `batch.edge_index`) through the model to get predictions (`out`). The specific model architecture and input structure depend on the problem.\n\n   - `out = torch.squeeze(out)`: This squeezes any unnecessary dimensions from the output tensor.\n\n   - `loss = loss_fn(out[batch.valid_mask], batch.y[batch.valid_mask])`: It calculates the loss between the model's predictions (`out`) and the ground truth targets (`batch.y`) but only for the valid data points (where `batch.valid_mask` is `True`).\n\n   - `mae = mae_fn(out[batch.valid_mask], batch.y[batch.valid_mask])`: Similarly, this calculates the mean absolute error (MAE) between predictions and targets for valid data points.\n\n   - `loss.backward()`: This computes gradients for the model's parameters with respect to the loss.\n\n   - `train_losses.append(loss.detach().cpu().numpy())` and `train_maes.append(mae.detach().cpu().numpy())`: These lines append the loss and MAE values (converted to NumPy arrays) for the current batch to the respective lists.\n\n   - `optimizer.step()`: This performs a parameter update using the computed gradients.\n\n   - `pbar.set_description(f\"Train loss {loss.detach().cpu().numpy():.4f}\")`: This updates the progress bar description to display the current training loss.\n\n7. After processing all batches in the training dataset, the code prints the average training loss and MAE for the current epoch.\n\n8. `val_losses = []` and `val_maes = []`: Similar to the training phase, these lines initialize empty lists to store validation losses and MAEs.\n\n9. `model.eval()`: This sets the model in evaluation mode, which disables operations like dropout during evaluation.\n\n10. The inner loop now iterates over batches in the validation dataset using the `val_dataloader`. It follows the same steps as the training loop but calculates and stores validation loss and MAE.\n\n11. After processing all validation batches, the code prints the average validation loss and MAE for the current epoch.\n\nThis code represents a typical training loop for supervised learning with neural networks, tracking model performance over multiple epochs and optimizing model parameters using gradient descent with the Adam optimizer. It also handles GPU acceleration if available (`batch = batch.to(device)`). .","metadata":{}},{"cell_type":"code","source":"n_epochs = 15\n\n# ğŸ“ˆ Define the optimizer with learning rate and weight decay\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=5e-4)\n\n# ğŸš‚ Iterate over epochs\nfor epoch in range(n_epochs):\n    train_losses = []\n    train_maes = []\n    model.train()\n    \n    # ğŸš Iterate over batches in the training dataloader\n    for batch in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index)\n        out = torch.squeeze(out)\n        loss = loss_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n        mae = mae_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n        loss.backward()\n        train_losses.append(loss.detach().cpu().numpy())\n        train_maes.append(mae.detach().cpu().numpy())\n        optimizer.step()\n        pbar.set_description(f\"Train loss {loss.detach().cpu().numpy():.4f}\")\n    \n    # ğŸ“Š Print average training loss and MAE for the epoch\n    print(f\"Epoch {epoch} train loss: \", np.mean(train_losses))\n    print(f\"Epoch {epoch} train mae: \", np.mean(train_maes))\n    \n    val_losses = []\n    val_maes = []\n    model.eval()\n    \n    # ğŸš Iterate over batches in the validation dataloader\n    for batch in (pbar := tqdm(val_dataloader, position=0, leave=True)):\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index)\n        out = torch.squeeze(out)\n        loss = loss_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n        mae = mae_fn(out[batch.valid_mask], batch.y[batch.valid_mask])\n        val_losses.append(loss.detach().cpu().numpy())\n        val_maes.append(mae.detach().cpu().numpy())\n        pbar.set_description(f\"Validation loss {loss.detach().cpu().numpy():.4f}\")\n    \n    # ğŸ“Š Print average validation loss and MAE for the epoch\n    print(f\"Epoch {epoch} val loss: \", np.mean(val_losses))\n    print(f\"Epoch {epoch} val mae: \", np.mean(val_maes))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T19:50:50.428934Z","iopub.execute_input":"2023-09-30T19:50:50.429282Z","iopub.status.idle":"2023-09-30T20:26:23.427945Z","shell.execute_reply.started":"2023-09-30T19:50:50.429255Z","shell.execute_reply":"2023-09-30T20:26:23.426404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ—‘ï¸ Clear the GPU memory cache\n","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:36.282054Z","iopub.execute_input":"2023-09-30T20:26:36.282408Z","iopub.status.idle":"2023-09-30T20:26:36.420452Z","shell.execute_reply.started":"2023-09-30T20:26:36.282379Z","shell.execute_reply":"2023-09-30T20:26:36.419239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“š Define a custom dataset class for inference on a graph\n","metadata":{}},{"cell_type":"markdown","source":"**Explaination**: \n\n","metadata":{}},{"cell_type":"code","source":"class InferenceGraphDataset(Dataset):\n    def __init__(self, parquet_name, edge_distance=2, root=None, transform=None, pre_transform=None, pre_filter=None):\n        super().__init__(root, transform, pre_transform, pre_filter)\n        # ğŸ“„ Set the Parquet file name\n        self.parquet_name = parquet_name\n        # ğŸ“ Set the edge distance for generating the adjacency matrix\n        self.edge_distance = edge_distance\n        # ğŸ§® Initialize the one-hot encoder for node features\n        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=4)\n        # ğŸ§® Fit the one-hot encoder to possible values (A, G, U, C)\n        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n        # ğŸ“Š Load the Parquet dataframe\n        self.df = pl.read_parquet(self.parquet_name)\n        # ğŸ“Š Select the 'sequence' and 'id_min' columns\n        self.sequence_df = self.df.select(\"sequence\")\n        self.id_min_df = self.df.select(\"id_min\")\n\n    def parse_row(self, idx):\n        # ğŸ“Š Read the row at the given index\n        sequence_row = self.sequence_df.row(idx)\n        id_min = self.id_min_df.row(idx)[0]\n\n        # ğŸ§¬ Get the sequence string and convert it to an array\n        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n        # ğŸ§¬ Encode the sequence array using the one-hot encoder\n        encoded_sequence = self.node_encoder.transform(sequence)\n        # ğŸ“ Get the sequence length\n        sequence_length = len(sequence)\n        # ğŸ“Š Get the edge index using nearest adjacency function\n        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n        # ğŸ“ Convert the edge index to a torch tensor\n        edge_index = torch.tensor(edges_np, dtype=torch.long)\n\n        # ğŸ“Š Define node features as the one-hot encoded sequence\n        node_features = torch.Tensor(encoded_sequence)\n        ids = torch.arange(id_min, id_min+sequence_length, 1)\n\n        data = Data(x=node_features, edge_index=edge_index, ids=ids)\n\n        return data\n\n    def len(self):\n        # ğŸ“ Return the length of the dataset\n        return len(self.df)\n\n    def get(self, idx):\n        # ğŸ“Š Get and parse data for the specified index\n        data = self.parse_row(idx)\n        return data\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:38.411662Z","iopub.execute_input":"2023-09-30T20:26:38.412035Z","iopub.status.idle":"2023-09-30T20:26:38.421806Z","shell.execute_reply.started":"2023-09-30T20:26:38.412007Z","shell.execute_reply":"2023-09-30T20:26:38.420951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“š Create an inference dataset and dataloader","metadata":{}},{"cell_type":"code","source":"infer_dataset = InferenceGraphDataset(parquet_name=TEST_PARQUET_FILE, edge_distance=EDGE_DISTANCE)  # ğŸš€ Inference dataset\ninfer_dataloader = DataLoader(infer_dataset, batch_size=128, shuffle=False, num_workers=2)  # ğŸ“¦ Inference dataloader\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:40.360240Z","iopub.execute_input":"2023-09-30T20:26:40.360575Z","iopub.status.idle":"2023-09-30T20:26:42.206202Z","shell.execute_reply.started":"2023-09-30T20:26:40.360549Z","shell.execute_reply":"2023-09-30T20:26:42.205194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ­ Set the device to GPU if available, otherwise use CPU\n","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# ğŸ­ Move the model to the selected device (GPU or CPU) and switch to evaluation mode\nmodel = model.eval().to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:46.131062Z","iopub.execute_input":"2023-09-30T20:26:46.131398Z","iopub.status.idle":"2023-09-30T20:26:46.138151Z","shell.execute_reply.started":"2023-09-30T20:26:46.131370Z","shell.execute_reply":"2023-09-30T20:26:46.137135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ§® Initialize empty arrays for IDs and predictions & Append IDs and predictions\n","metadata":{}},{"cell_type":"code","source":"ids = np.empty(shape=(0, 1), dtype=int)\npreds = np.empty(shape=(0, 1), dtype=np.float32)\n\n# ğŸš€ Iterate over batches in the inference dataloader\nfor batch in tqdm(infer_dataloader):\n    batch = batch.to(device)\n    out = model(batch.x, batch.edge_index).detach().cpu().numpy()\n\n    # ğŸ“¦ Append IDs and predictions to the respective arrays\n    ids = np.append(ids, batch.ids.detach().cpu().numpy())\n    preds = np.append(preds, out)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T20:26:48.298832Z","iopub.execute_input":"2023-09-30T20:26:48.299212Z","iopub.status.idle":"2023-09-30T22:16:43.862235Z","shell.execute_reply.started":"2023-09-30T20:26:48.299184Z","shell.execute_reply":"2023-09-30T22:16:43.860996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ“Š Create a DataFrame for the submission\n","metadata":{}},{"cell_type":"code","source":"submission_df = pl.DataFrame({\"id\": ids, \"reactivity_DMS_MaP\": preds, \"reactivity_2A3_MaP\": preds})\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T22:17:27.749228Z","iopub.execute_input":"2023-09-30T22:17:27.749614Z","iopub.status.idle":"2023-09-30T22:17:30.957824Z","shell.execute_reply.started":"2023-09-30T22:17:27.749583Z","shell.execute_reply":"2023-09-30T22:17:30.956821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ’¾ Write the submission DataFrame to a CSV file\n","metadata":{}},{"cell_type":"code","source":"submission_df.write_csv(PRED_CSV)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T22:17:48.424485Z","iopub.execute_input":"2023-09-30T22:17:48.424848Z","iopub.status.idle":"2023-09-30T22:35:34.221809Z","shell.execute_reply.started":"2023-09-30T22:17:48.424819Z","shell.execute_reply":"2023-09-30T22:35:34.220719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore More! ğŸ‘€\nThank you for exploring this notebook! If you found this notebook insightful or if it helped you in any way, I invite you to explore more of my work on my profile.\n\nğŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainali) ğŸ‘ˆ\n\n## Feedback and Gratitude ğŸ™\nWe value your feedback! Your insights and suggestions are essential for our continuous improvement. If you have any comments, questions, or ideas to share, please don't hesitate to reach out.\n\nğŸ“¬ Contact me via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n\nI would like to express our heartfelt gratitude for your time and engagement. Your support motivates us to create more valuable content.\n\nHappy coding and best of luck in your data science endeavors! ğŸš€\n","metadata":{}}]}
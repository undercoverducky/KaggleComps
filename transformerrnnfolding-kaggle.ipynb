{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0ff1e6",
   "metadata": {
    "papermill": {
     "duration": 0.005636,
     "end_time": "2023-10-04T22:15:25.425398",
     "exception": false,
     "start_time": "2023-10-04T22:15:25.419762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MHA Transformer \n",
    "\n",
    "## Introduction\n",
    "Welcome to this Jupyter notebook developed for Stanford Ribonanza RNA Folding to create a model that predicts the structures of any RNA molecule using sequence to sequence NLP techniques.\n",
    "\n",
    "## Purpose\n",
    "The primary purpose of this notebook is to:\n",
    "- Load and preprocess the competition data 📁\n",
    "- Engineer relevant features for model training 🏋️‍♂️\n",
    "- Train predictive models to make target variable predictions 🧠\n",
    "- Submit predictions to the competition environment 📤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b368c58",
   "metadata": {
    "papermill": {
     "duration": 0.004885,
     "end_time": "2023-10-04T22:15:25.435599",
     "exception": false,
     "start_time": "2023-10-04T22:15:25.430714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📦 Import necessary libraries and modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108cd583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:15:25.447144Z",
     "iopub.status.busy": "2023-10-04T22:15:25.446716Z",
     "iopub.status.idle": "2023-10-04T22:15:31.914473Z",
     "shell.execute_reply": "2023-10-04T22:15:31.913569Z"
    },
    "papermill": {
     "duration": 6.476161,
     "end_time": "2023-10-04T22:15:31.916760",
     "exception": false,
     "start_time": "2023-10-04T22:15:25.440599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch  # 🔥 Import PyTorch for deep learning\n",
    "from torch.utils.data import random_split  # 📂 For splitting datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd  # 🐼 Pandas for data manipulation\n",
    "from pathlib import Path  # 📁 pathlib for handling file paths\n",
    "import numpy as np  # 🧮 NumPy for numerical operations\n",
    "from sklearn.preprocessing import OneHotEncoder  # 🧬 Scikit-Learn for one-hot encoding\n",
    "import polars as pl  # 📊 Polars for data manipulation\n",
    "import re  # 🧵 Regular expressions for text processing\n",
    "from tqdm import tqdm  # 🔄 tqdm for progress bar display\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d8303",
   "metadata": {
    "papermill": {
     "duration": 0.005,
     "end_time": "2023-10-04T22:15:31.927357",
     "exception": false,
     "start_time": "2023-10-04T22:15:31.922357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📁 Define file paths for dataset and output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ec1f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:15:31.938959Z",
     "iopub.status.busy": "2023-10-04T22:15:31.938568Z",
     "iopub.status.idle": "2023-10-04T22:15:31.943218Z",
     "shell.execute_reply": "2023-10-04T22:15:31.942247Z"
    },
    "papermill": {
     "duration": 0.012342,
     "end_time": "2023-10-04T22:15:31.944871",
     "exception": false,
     "start_time": "2023-10-04T22:15:31.932529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/kaggle/input/stanford-ribonanza-rna-folding/\")  # 📂 Directory for dataset\n",
    "TRAIN_CSV = DATA_DIR / \"train_data.csv\"  # 🚆 Training data in CSV format\n",
    "TRAIN_PARQUET_FILE = \"train_data.parquet\"  # 📦 Training data in Parquet format\n",
    "TEST_CSV = DATA_DIR / \"test_sequences.csv\"  # 🚀 Test sequences in CSV format\n",
    "TEST_PARQUET_FILE = \"test_sequences.parquet\"  # 📦 Test sequences in Parquet format\n",
    "PRED_CSV = \"submission.csv\"  # 📄 Output file for predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea035f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:15:31.956557Z",
     "iopub.status.busy": "2023-10-04T22:15:31.955807Z",
     "iopub.status.idle": "2023-10-04T22:15:31.961481Z",
     "shell.execute_reply": "2023-10-04T22:15:31.960698Z"
    },
    "papermill": {
     "duration": 0.013253,
     "end_time": "2023-10-04T22:15:31.963177",
     "exception": false,
     "start_time": "2023-10-04T22:15:31.949924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_parquet(csv_file, parquet_file):\n",
    "    # 📊 Read CSV data using Polars\n",
    "    dummy_df = pl.scan_csv(csv_file)\n",
    "\n",
    "    # 🔍 Define a new schema mapping for specific columns\n",
    "    new_schema = {}\n",
    "    for key, value in dummy_df.schema.items():\n",
    "        if key.startswith(\"reactivity\"):\n",
    "            new_schema[key] = pl.Float32  # 📊 Convert 'reactivity' columns to Float32\n",
    "        else:\n",
    "            new_schema[key] = value\n",
    "\n",
    "    # 📊 Read CSV data with the new schema and write to Parquet\n",
    "    df = pl.scan_csv(csv_file, schema=new_schema)\n",
    "    \n",
    "    # 💾 Write data to Parquet format with specified settings\n",
    "    df.sink_parquet(\n",
    "        parquet_file,\n",
    "        compression='uncompressed',  # No compression for easy access\n",
    "        row_group_size=10,  # Adjust row group size as needed\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c036f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:15:31.974771Z",
     "iopub.status.busy": "2023-10-04T22:15:31.974020Z",
     "iopub.status.idle": "2023-10-04T22:18:17.727251Z",
     "shell.execute_reply": "2023-10-04T22:18:17.726263Z"
    },
    "papermill": {
     "duration": 165.761318,
     "end_time": "2023-10-04T22:18:17.729493",
     "exception": false,
     "start_time": "2023-10-04T22:15:31.968175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_parquet(TRAIN_CSV, TRAIN_PARQUET_FILE)  # 🚆 Training data\n",
    "to_parquet(TEST_CSV, TEST_PARQUET_FILE)    # 🚀 Test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28bd69a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:17.742757Z",
     "iopub.status.busy": "2023-10-04T22:18:17.741935Z",
     "iopub.status.idle": "2023-10-04T22:18:17.754957Z",
     "shell.execute_reply": "2023-10-04T22:18:17.754142Z"
    },
    "papermill": {
     "duration": 0.021281,
     "end_time": "2023-10-04T22:18:17.756637",
     "exception": false,
     "start_time": "2023-10-04T22:18:17.735356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Indexer(object):\n",
    "    \"\"\"\n",
    "    Bijection between objects and integers starting at 0. Useful for mapping\n",
    "    labels, features, etc. into coordinates of a vector space.\n",
    "\n",
    "    Attributes:\n",
    "        objs_to_ints\n",
    "        ints_to_objs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    def add_and_get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, parquet_name, sequence_index, context_length=50):\n",
    "            \n",
    "        self.parquet_name = parquet_name\n",
    "        self.seq_indexer = sequence_index\n",
    "        self.df = pl.read_parquet(self.parquet_name)\n",
    "        self.df = self.df.filter(pl.col(\"SN_filter\") == 1.0)\n",
    "        # 🧬 Get reactivity column names using regular expression\n",
    "        reactivity_match = re.compile('(reactivity_[0-9])')\n",
    "        reactivity_names = [col for col in self.df.columns if reactivity_match.match(col)]\n",
    "        # 📊 Select only the reactivity columns\n",
    "        self.reactivity_df = self.df.select(reactivity_names)\n",
    "        # 📊 Select the 'sequence' column\n",
    "        self.sequence_df = self.df.select(\"sequence\")\n",
    "        self.context_length = context_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def parse_row(self, idx):\n",
    "        sequence_row = self.sequence_df.row(idx)\n",
    "        reactivity_row = self.reactivity_df.row(idx)\n",
    "        # 🧬 Get the sequence string\n",
    "        sequence = list(sequence_row[0])\n",
    "        # 🧬 Encode the sequence array using the one-hot encoder\n",
    "        sequence_indices = torch.LongTensor(np.array([self.seq_indexer.index_of(s) for s in sequence]))\n",
    "        sequence_length = len(sequence)\n",
    "        start_idx = random.randint(0, len(sequence) - context_length)\n",
    "        selected_sequence = sequence_indices[start_idx : start_idx + context_length]\n",
    "        reactivity = torch.LongTensor(np.array(reactivity_row, dtype=np.float32)[start_idx : start_idx + context_length])\n",
    "        valid_mask = np.argwhere(~np.isnan(reactivity)).reshape(-1)\n",
    "        torch_valid_mask = torch.tensor(valid_mask, dtype=torch.long)\n",
    "        # 🧬 Replace nan values for reactivity with 0.0 TODO figure out how to smoth these, perhaps with information from other columns\n",
    "        reactivity = np.nan_to_num(reactivity, copy=False, nan=0.0)\n",
    "        \n",
    "        return selected_sequence, reactivity, valid_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 📊 Get and parse data for the specified index\n",
    "        data = self.parse_row(idx)\n",
    "        return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dada7c",
   "metadata": {
    "papermill": {
     "duration": 0.004986,
     "end_time": "2023-10-04T22:18:17.766776",
     "exception": false,
     "start_time": "2023-10-04T22:18:17.761790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📚 Create the full training dataset and split it into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c800fdd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:17.779309Z",
     "iopub.status.busy": "2023-10-04T22:18:17.778709Z",
     "iopub.status.idle": "2023-10-04T22:18:33.174907Z",
     "shell.execute_reply": "2023-10-04T22:18:33.172813Z"
    },
    "papermill": {
     "duration": 15.406947,
     "end_time": "2023-10-04T22:18:33.179189",
     "exception": false,
     "start_time": "2023-10-04T22:18:17.772242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'G', 'U', 'C']\n"
     ]
    }
   ],
   "source": [
    "vocab = ['A', 'G', 'U', 'C']\n",
    "vocab_index = Indexer()\n",
    "for char in vocab:\n",
    "    vocab_index.add_and_get_index(char)\n",
    "print(repr(vocab_index))\n",
    "\n",
    "full_train_dataset = SequenceDataset(parquet_name=TRAIN_PARQUET_FILE, sequence_index=vocab_index, context_length=80)  # 🚆 Full training dataset\n",
    "generator1 = torch.Generator().manual_seed(42)  # 🌱 Initialize random seed generator\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [0.7, 0.3], generator1)  # 🎯 Split dataset into training (70%) and validation (30%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443b8d3",
   "metadata": {
    "papermill": {
     "duration": 0.015002,
     "end_time": "2023-10-04T22:18:33.210718",
     "exception": false,
     "start_time": "2023-10-04T22:18:33.195716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🚂 Create data loaders for training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c50bd5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:33.244244Z",
     "iopub.status.busy": "2023-10-04T22:18:33.243750Z",
     "iopub.status.idle": "2023-10-04T22:18:33.256163Z",
     "shell.execute_reply": "2023-10-04T22:18:33.255115Z"
    },
    "papermill": {
     "duration": 0.03316,
     "end_time": "2023-10-04T22:18:33.259625",
     "exception": false,
     "start_time": "2023-10-04T22:18:33.226465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)  # 📦 Training data loader\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0)  # 📦 Validation data loader\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36f03a",
   "metadata": {
    "papermill": {
     "duration": 0.0154,
     "end_time": "2023-10-04T22:18:33.290709",
     "exception": false,
     "start_time": "2023-10-04T22:18:33.275309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📉 Define loss functions for training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea252381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:33.324527Z",
     "iopub.status.busy": "2023-10-04T22:18:33.324076Z",
     "iopub.status.idle": "2023-10-04T22:18:33.336262Z",
     "shell.execute_reply": "2023-10-04T22:18:33.335216Z"
    },
    "papermill": {
     "duration": 0.032717,
     "end_time": "2023-10-04T22:18:33.339650",
     "exception": false,
     "start_time": "2023-10-04T22:18:33.306933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 📉 Define loss functions for training and evaluation\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_fn(output, target):\n",
    "    # 🪟 Clip the target values to be within the range [0, 1]\n",
    "    clipped_target = torch.clip(target, min=0, max=1)\n",
    "    # 📉 Calculate the mean squared error loss\n",
    "    mses = F.mse_loss(output, clipped_target, reduction='mean')\n",
    "    return mses\n",
    "\n",
    "def mae_fn(output, target):\n",
    "    # 🪟 Clip the target values to be within the range [0, 1]\n",
    "    clipped_target = torch.clip(target, min=0, max=1)\n",
    "    # 📉 Calculate the mean absolute error loss\n",
    "    maes = F.l1_loss(output, clipped_target, reduction='mean')\n",
    "    return maes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d54e8",
   "metadata": {
    "papermill": {
     "duration": 0.015528,
     "end_time": "2023-10-04T22:18:33.370862",
     "exception": false,
     "start_time": "2023-10-04T22:18:33.355334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🧠 Create a neural network model using Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c29af89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:33.404034Z",
     "iopub.status.busy": "2023-10-04T22:18:33.403332Z",
     "iopub.status.idle": "2023-10-04T22:18:33.457488Z",
     "shell.execute_reply": "2023-10-04T22:18:33.456098Z"
    },
    "papermill": {
     "duration": 0.07476,
     "end_time": "2023-10-04T22:18:33.460969",
     "exception": false,
     "start_time": "2023-10-04T22:18:33.386209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, seq_length, d_model, num_heads, d_internal):\n",
    "        super().__init__()\n",
    "        self.K = nn.Linear(d_model, d_internal)\n",
    "        self.Q = nn.Linear(d_model, d_internal)\n",
    "        self.V = nn.Linear(d_model, d_internal)\n",
    "        self.w0 = nn.Linear(d_internal, d_model // num_heads)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(seq_length, seq_length)))\n",
    "\n",
    "    def forward(self, input_vecs):\n",
    "        keys = self.K(input_vecs) # B, L, d_internal\n",
    "        d_k = keys.shape[-1]\n",
    "        queries = self.Q(input_vecs) # B, L, d_internal\n",
    "        value = self.V(input_vecs) # B, L, d_internal\n",
    "        weights = torch.matmul(queries, keys.transpose(-2, -1)) * d_k**-0.5# L, L\n",
    "        weights = weights.masked_fill(self.tril == 0, float('-inf'))\n",
    "        attention = torch.softmax(weights, dim=-1)\n",
    "\n",
    "        logit = torch.matmul(attention , value) # B, L, d_internal\n",
    "        logit = self.w0(logit)\n",
    "        return logit\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_length, d_model, num_heads, d_internal):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(seq_length, d_model, num_heads, d_internal) for _ in range(num_heads)])\n",
    "        self.linear1 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, input_vecs):\n",
    "        out = torch.cat([head(input_vecs) for head in self.heads], dim=-1)\n",
    "        out = self.linear1(out)\n",
    "        return out\n",
    "\n",
    "class MHATransformerLayer(nn.Module):\n",
    "    def __init__(self, seq_length, d_model, num_heads, d_internal):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention( seq_length, d_model, num_heads, d_internal)\n",
    "        self.ffwd = FeedFoward(d_model)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input_vecs):\n",
    "        x = self.multi_head_attention(self.ln1(input_vecs))\n",
    "        x += input_vecs\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class MHATransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, num_positions, d_model, d_internal, num_layers, num_heads):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.num_positions = num_positions\n",
    "        self.L = []\n",
    "        for ly in range(num_layers):\n",
    "            self.L.append(MHATransformerLayer(num_positions, d_model, num_heads, d_internal))\n",
    "        self.transformer_layers = nn.Sequential(*self.L)\n",
    "        self.classifier = nn.Linear(d_model, 1)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, indices, batched=False):\n",
    "        logit = self.embedding(indices)\n",
    "        logit = self.transformer_layers(logit)\n",
    "        logit = self.classifier(logit)\n",
    "        logit = self.softmax(logit)\n",
    "        if batched:\n",
    "            return logit\n",
    "        else:\n",
    "            return logit.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff335447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:33.495614Z",
     "iopub.status.busy": "2023-10-04T22:18:33.494951Z",
     "iopub.status.idle": "2023-10-04T22:18:37.423695Z",
     "shell.execute_reply": "2023-10-04T22:18:37.422331Z"
    },
    "papermill": {
     "duration": 3.950025,
     "end_time": "2023-10-04T22:18:37.427667",
     "exception": false,
     "start_time": "2023-10-04T22:18:33.477642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🛠️ Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "context_length = 80\n",
    "model = MHATransformer(vocab_size=4, num_positions=context_length, d_model=64, d_internal=32, num_layers=1, num_heads=8).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c4c817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.479685Z",
     "iopub.status.busy": "2023-10-04T22:18:37.479248Z",
     "iopub.status.idle": "2023-10-04T22:18:37.493063Z",
     "shell.execute_reply": "2023-10-04T22:18:37.492026Z"
    },
    "papermill": {
     "duration": 0.051561,
     "end_time": "2023-10-04T22:18:37.496356",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.444795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# Make sure we are using the GPU\n",
    "device\n",
    "#print(len(train_dataloader))\n",
    "#for  batch_x, batch_y, valid_mask in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n",
    "#    print(batch_x.shape)\n",
    "#    print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc9c42c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.554533Z",
     "iopub.status.busy": "2023-10-04T22:18:37.554036Z",
     "iopub.status.idle": "2023-10-04T22:18:37.574662Z",
     "shell.execute_reply": "2023-10-04T22:18:37.573626Z"
    },
    "papermill": {
     "duration": 0.058436,
     "end_time": "2023-10-04T22:18:37.577478",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.519042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 0\n",
    "\n",
    "# 📈 Define the optimizer with learning rate and weight decay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "\n",
    "# 🚂 Iterate over epochs\n",
    "for epoch in range(n_epochs):\n",
    "    train_losses = []\n",
    "    train_maes = []\n",
    "    model.train()\n",
    "    \n",
    "    # 🚞 Iterate over batches in the training dataloader\n",
    "    for  batch_x, batch_y, valid_mask in (pbar := tqdm(train_dataloader, position=0, leave=True)):\n",
    "        batch_x, batch_y, valid_mask = batch_x.to(device), batch_y.to(device), valid_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch_x, batched=True)\n",
    "        out = torch.squeeze(out)\n",
    "        loss = loss_fn(out[valid_mask], batch_y[valid_mask].float())\n",
    "        mae = mae_fn(out[valid_mask], batch_y[valid_mask])\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.detach().cpu().numpy())\n",
    "        train_maes.append(mae.detach().cpu().numpy())\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Train loss {loss.detach().cpu().numpy():.4f}\")\n",
    "    \n",
    "    # 📊 Print average training loss and MAE for the epoch\n",
    "    print(f\"Epoch {epoch} train loss: \", np.mean(train_losses))\n",
    "    print(f\"Epoch {epoch} train mae: \", np.mean(train_maes))\n",
    "    \n",
    "    val_losses = []\n",
    "    val_maes = []\n",
    "    model.eval()\n",
    "    \n",
    "    # 🚞 Iterate over batches in the validation dataloader\n",
    "    for batch_x, batch_y, valid_mask in (pbar := tqdm(val_dataloader, position=0, leave=True)):\n",
    "        batch_x, batch_y, valid_mask = batch_x.to(device), batch_y.to(device), valid_mask.to(device)\n",
    "        out = model(batch_x, batched=True)\n",
    "        out = torch.squeeze(out)\n",
    "        loss = loss_fn(out[valid_mask], batch_y[valid_mask].float())\n",
    "        mae = mae_fn(out[valid_mask], batch_y[valid_mask])\n",
    "        val_losses.append(loss.detach().cpu().numpy())\n",
    "        val_maes.append(mae.detach().cpu().numpy())\n",
    "        pbar.set_description(f\"Validation loss {loss.detach().cpu().numpy():.4f}\")\n",
    "    \n",
    "    # 📊 Print average validation loss and MAE for the epoch\n",
    "    print(f\"Epoch {epoch} val loss: \", np.mean(val_losses))\n",
    "    print(f\"Epoch {epoch} val mae: \", np.mean(val_maes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3764a2",
   "metadata": {
    "papermill": {
     "duration": 0.014405,
     "end_time": "2023-10-04T22:18:37.623341",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.608936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🗑️ Clear the GPU memory cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ae78ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.674256Z",
     "iopub.status.busy": "2023-10-04T22:18:37.673916Z",
     "iopub.status.idle": "2023-10-04T22:18:37.678199Z",
     "shell.execute_reply": "2023-10-04T22:18:37.677254Z"
    },
    "papermill": {
     "duration": 0.012523,
     "end_time": "2023-10-04T22:18:37.679901",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.667378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924a7c5",
   "metadata": {
    "papermill": {
     "duration": 0.005063,
     "end_time": "2023-10-04T22:18:37.690266",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.685203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📚 Define a custom dataset class for inference on a graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab2a40",
   "metadata": {
    "papermill": {
     "duration": 0.005096,
     "end_time": "2023-10-04T22:18:37.700705",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.695609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "648376b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.712729Z",
     "iopub.status.busy": "2023-10-04T22:18:37.712494Z",
     "iopub.status.idle": "2023-10-04T22:18:37.720562Z",
     "shell.execute_reply": "2023-10-04T22:18:37.719705Z"
    },
    "papermill": {
     "duration": 0.01623,
     "end_time": "2023-10-04T22:18:37.722179",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.705949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceGraphDataset(Dataset):\n",
    "    def __init__(self, parquet_name, edge_distance=2, root=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        # 📄 Set the Parquet file name\n",
    "        self.parquet_name = parquet_name\n",
    "        # 📏 Set the edge distance for generating the adjacency matrix\n",
    "        self.edge_distance = edge_distance\n",
    "        # 🧮 Initialize the one-hot encoder for node features\n",
    "        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=4)\n",
    "        # 🧮 Fit the one-hot encoder to possible values (A, G, U, C)\n",
    "        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n",
    "        # 📊 Load the Parquet dataframe\n",
    "        self.df = pl.read_parquet(self.parquet_name)\n",
    "        # 📊 Select the 'sequence' and 'id_min' columns\n",
    "        self.sequence_df = self.df.select(\"sequence\")\n",
    "        self.id_min_df = self.df.select(\"id_min\")\n",
    "\n",
    "    def parse_row(self, idx):\n",
    "        # 📊 Read the row at the given index\n",
    "        sequence_row = self.sequence_df.row(idx)\n",
    "        id_min = self.id_min_df.row(idx)[0]\n",
    "\n",
    "        # 🧬 Get the sequence string and convert it to an array\n",
    "        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n",
    "        # 🧬 Encode the sequence array using the one-hot encoder\n",
    "        encoded_sequence = self.node_encoder.transform(sequence)\n",
    "        # 📏 Get the sequence length\n",
    "        sequence_length = len(sequence)\n",
    "        # 📊 Get the edge index using nearest adjacency function\n",
    "        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n",
    "        # 📏 Convert the edge index to a torch tensor\n",
    "        edge_index = torch.tensor(edges_np, dtype=torch.long)\n",
    "\n",
    "        # 📊 Define node features as the one-hot encoded sequence\n",
    "        node_features = torch.Tensor(encoded_sequence)\n",
    "        ids = torch.arange(id_min, id_min+sequence_length, 1)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, ids=ids)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def len(self):\n",
    "        # 📏 Return the length of the dataset\n",
    "        return len(self.df)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # 📊 Get and parse data for the specified index\n",
    "        data = self.parse_row(idx)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9931a2",
   "metadata": {
    "papermill": {
     "duration": 0.005491,
     "end_time": "2023-10-04T22:18:37.733123",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.727632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📚 Create an inference dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7e98b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.744996Z",
     "iopub.status.busy": "2023-10-04T22:18:37.744742Z",
     "iopub.status.idle": "2023-10-04T22:18:37.748342Z",
     "shell.execute_reply": "2023-10-04T22:18:37.747502Z"
    },
    "papermill": {
     "duration": 0.011297,
     "end_time": "2023-10-04T22:18:37.749959",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.738662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#infer_dataset = InferenceGraphDataset(parquet_name=TEST_PARQUET_FILE, edge_distance=EDGE_DISTANCE)  # 🚀 Inference dataset\n",
    "#infer_dataloader = DataLoader(infer_dataset, batch_size=128, shuffle=False, num_workers=2)  # 📦 Inference dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe68e3",
   "metadata": {
    "papermill": {
     "duration": 0.005243,
     "end_time": "2023-10-04T22:18:37.760459",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.755216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🏭 Set the device to GPU if available, otherwise use CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d3374f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.772093Z",
     "iopub.status.busy": "2023-10-04T22:18:37.771781Z",
     "iopub.status.idle": "2023-10-04T22:18:37.775643Z",
     "shell.execute_reply": "2023-10-04T22:18:37.774887Z"
    },
    "papermill": {
     "duration": 0.011577,
     "end_time": "2023-10-04T22:18:37.777311",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.765734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 🏭 Move the model to the selected device (GPU or CPU) and switch to evaluation mode\n",
    "#model = model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6526fab",
   "metadata": {
    "papermill": {
     "duration": 0.00528,
     "end_time": "2023-10-04T22:18:37.827895",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.822615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 🧮 Initialize empty arrays for IDs and predictions & Append IDs and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "409c62b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.841007Z",
     "iopub.status.busy": "2023-10-04T22:18:37.840187Z",
     "iopub.status.idle": "2023-10-04T22:18:37.845077Z",
     "shell.execute_reply": "2023-10-04T22:18:37.844287Z"
    },
    "papermill": {
     "duration": 0.013346,
     "end_time": "2023-10-04T22:18:37.846677",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.833331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ids = np.empty(shape=(0, 1), dtype=int)\n",
    "#preds = np.empty(shape=(0, 1), dtype=np.float32)\n",
    "\n",
    "# 🚀 Iterate over batches in the inference dataloader\n",
    "#for batch in tqdm(infer_dataloader):\n",
    "#    batch = batch.to(device)\n",
    "#    out = model(batch.x, batch.edge_index).detach().cpu().numpy()\n",
    "\n",
    "    # 📦 Append IDs and predictions to the respective arrays\n",
    "#    ids = np.append(ids, batch.ids.detach().cpu().numpy())\n",
    "#    preds = np.append(preds, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328ce3d",
   "metadata": {
    "papermill": {
     "duration": 0.005122,
     "end_time": "2023-10-04T22:18:37.857116",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.851994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📊 Create a DataFrame for the submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6369c98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.868963Z",
     "iopub.status.busy": "2023-10-04T22:18:37.868355Z",
     "iopub.status.idle": "2023-10-04T22:18:37.872111Z",
     "shell.execute_reply": "2023-10-04T22:18:37.871153Z"
    },
    "papermill": {
     "duration": 0.011358,
     "end_time": "2023-10-04T22:18:37.873725",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.862367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#submission_df = pl.DataFrame({\"id\": ids, \"reactivity_DMS_MaP\": preds, \"reactivity_2A3_MaP\": preds})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f684e",
   "metadata": {
    "papermill": {
     "duration": 0.005252,
     "end_time": "2023-10-04T22:18:37.884335",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.879083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 💾 Write the submission DataFrame to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e48a1957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.896235Z",
     "iopub.status.busy": "2023-10-04T22:18:37.895898Z",
     "iopub.status.idle": "2023-10-04T22:18:37.899489Z",
     "shell.execute_reply": "2023-10-04T22:18:37.898609Z"
    },
    "papermill": {
     "duration": 0.011334,
     "end_time": "2023-10-04T22:18:37.901149",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.889815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#submission_df.write_csv(PRED_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "955a6775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T22:18:37.913559Z",
     "iopub.status.busy": "2023-10-04T22:18:37.913043Z",
     "iopub.status.idle": "2023-10-04T22:18:37.923151Z",
     "shell.execute_reply": "2023-10-04T22:18:37.922305Z"
    },
    "papermill": {
     "duration": 0.018333,
     "end_time": "2023-10-04T22:18:37.924870",
     "exception": false,
     "start_time": "2023-10-04T22:18:37.906537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleGraphDataset(Dataset):\n",
    "    def __init__(self, parquet_name, edge_distance=5, root=None, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        # 📄 Set the Parquet file name\n",
    "        self.parquet_name = parquet_name\n",
    "        # 📏 Set the edge distance for generating the adjacency matrix\n",
    "        self.edge_distance = edge_distance\n",
    "        # 🧮 Initialize the one-hot encoder for node features\n",
    "        self.node_encoder = OneHotEncoder(sparse_output=False, max_categories=5)\n",
    "        # 🧮 Fit the one-hot encoder to possible values (A, G, U, C)\n",
    "        self.node_encoder.fit(np.array(['A', 'G', 'U', 'C']).reshape(-1, 1))\n",
    "        # 📊 Load the Parquet dataframe\n",
    "        self.df = pl.read_parquet(self.parquet_name)\n",
    "        # 📊 Filter the dataframe by 'SN_filter' column where the value is 1.0\n",
    "        self.df = self.df.filter(pl.col(\"SN_filter\") == 1.0)\n",
    "        # 🧬 Get reactivity column names using regular expression\n",
    "        reactivity_match = re.compile('(reactivity_[0-9])')\n",
    "        reactivity_names = [col for col in self.df.columns if reactivity_match.match(col)]\n",
    "        # 📊 Select only the reactivity columns\n",
    "        self.reactivity_df = self.df.select(reactivity_names)\n",
    "        # 📊 Select the 'sequence' column\n",
    "        self.sequence_df = self.df.select(\"sequence\")\n",
    "        \n",
    "\n",
    "    def parse_row(self, idx):\n",
    "        # 📊 Read the row at the given index\n",
    "        sequence_row = self.sequence_df.row(idx)\n",
    "        reactivity_row = self.reactivity_df.row(idx)\n",
    "        # 🧬 Get the sequence string and convert it to an array\n",
    "        sequence = np.array(list(sequence_row[0])).reshape(-1, 1)\n",
    "        # 🧬 Encode the sequence array using the one-hot encoder\n",
    "        encoded_sequence = self.node_encoder.transform(sequence)\n",
    "        # 📏 Get the sequence length\n",
    "        sequence_length = len(sequence)\n",
    "        # 📊 Get the edge index using nearest adjacency function\n",
    "        edges_np = nearest_adjacency(sequence_length, n=self.edge_distance, loops=False)\n",
    "        # 📏 Convert the edge index to a torch tensor\n",
    "        edge_index = torch.tensor(edges_np, dtype=torch.long)\n",
    "        # 🧬 Get reactivity targets for nodes\n",
    "        reactivity = np.array(reactivity_row, dtype=np.float32)[0:sequence_length]\n",
    "        # 🔒 Create valid masks for nodes\n",
    "        valid_mask = np.argwhere(~np.isnan(reactivity)).reshape(-1)\n",
    "        torch_valid_mask = torch.tensor(valid_mask, dtype=torch.long)\n",
    "        # 🧬 Replace nan values for reactivity with 0.0 (not super important as they get masked)\n",
    "        reactivity = np.nan_to_num(reactivity, copy=False, nan=0.0)\n",
    "        # 📊 Define node features as the one-hot encoded sequence\n",
    "        node_features = torch.Tensor(encoded_sequence)\n",
    "        # 🎯 Define targets\n",
    "        targets = torch.Tensor(reactivity)\n",
    "        # 📊 Create a PyTorch Data object\n",
    "        data = Data(x=node_features, edge_index=edge_index, y=targets, valid_mask=torch_valid_mask)\n",
    "        return data\n",
    "\n",
    "    def len(self):\n",
    "        # 📏 Return the length of the dataset\n",
    "        return len(self.df)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # 📊 Get and parse data for the specified index\n",
    "        data = self.parse_row(idx)\n",
    "        return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 198.758909,
   "end_time": "2023-10-04T22:18:40.839673",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-04T22:15:22.080764",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
